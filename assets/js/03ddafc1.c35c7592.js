"use strict";(globalThis.webpackChunkhemihex_docs=globalThis.webpackChunkhemihex_docs||[]).push([[8937],{28453:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>o});var l=a(96540);const s={},i=l.createContext(s);function r(e){const n=l.useContext(i);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),l.createElement(i.Provider,{value:n},e.children)}},91624:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>t,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>c});const l=JSON.parse('{"id":"HH-101/Offline AI Model/multimodal-visual-understanding-speech-interaction-11-offlineaimodel-11-20","title":"Multimodal Visual Understanding Speech Interaction","description":"1. Concept Introduction","source":"@site/docs/hh101/HH-101/11- Offline AI Model/20-multimodal-visual-understanding-speech-interaction-11-offlineaimodel-11-20.md","sourceDirName":"HH-101/11- Offline AI Model","slug":"/HH-101/Offline AI Model/multimodal-visual-understanding-speech-interaction-11-offlineaimodel-11-20","permalink":"/hh-101/HH-101/Offline AI Model/multimodal-visual-understanding-speech-interaction-11-offlineaimodel-11-20","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"title":"Multimodal Visual Understanding Speech Interaction","sidebar_position":0},"sidebar":"hh101Sidebar","previous":{"title":"Offline Text to Speech (TTS)","permalink":"/hh-101/HH-101/Offline AI Model/offline-text-to-speech-tts-11-offlineaimodel-11-19"},"next":{"title":"Multimodal Text and Graphics Application","permalink":"/hh-101/HH-101/Offline AI Model/multimodal-text-and-graphics-application-11-offlineaimodel-11-21"}}');var s=a(74848),i=a(28453);const r={title:"Multimodal Visual Understanding Speech Interaction",sidebar_position:0},o="4.Multimodal visual understand speech interaction",t={},c=[{value:"1. Concept Introduction",id:"1-concept-introduction",level:2},{value:"1.1 What is &quot;Visual Understanding&quot;?",id:"11-what-is-visual-understanding",level:3},{value:"1.2 Implementation Principle Overview",id:"12-implementation-principle-overview",level:3},{value:"2. Code Explanation",id:"2-code-explanation",level:2},{value:"Key Code",id:"key-code",level:3},{value:"1. Tools Layer Entry ( largemodel/utils/tools_manager.py )",id:"1-tools-layer-entry--largemodelutilstools_managerpy-",level:4},{value:"2. Model interface layer ( largemodel/utils/large_model_interface.py )",id:"2-model-interface-layer--largemodelutilslarge_model_interfacepy-",level:4},{value:"Code Analysis",id:"code-analysis",level:3},{value:"3.1 Configuring the Offline Large Model",id:"31-configuring-the-offline-large-model",level:2},{value:"3.1.1 Configuring the LLM Platform ( HemiHex.yaml )",id:"311-configuring-the-llm-platform--hemihexyaml-",level:4},{value:"3.1.2 Configuring the Model Interface ( large_model_interface.yaml )",id:"312-configuring-the-model-interface--large_model_interfaceyaml-",level:4},{value:"3.2 Starting and Testing the Function",id:"32-starting-and-testing-the-function",level:3},{value:"4. Common Problems and Solutions",id:"4-common-problems-and-solutions",level:2},{value:"4.1 Very Slow Responses",id:"41-very-slow-responses",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"4multimodal-visual-understand-speech-interaction",children:"4.Multimodal visual understand speech interaction"})}),"\n",(0,s.jsx)(n.h2,{id:"1-concept-introduction",children:"1. Concept Introduction"}),"\n",(0,s.jsx)(n.h3,{id:"11-what-is-visual-understanding",children:'1.1 What is "Visual Understanding"?'}),"\n",(0,s.jsx)(n.p,{children:'In the largemodel project, the multimodal visual understanding feature enables robots to go beyond simply "seeing" a matrix of pixels and truly "understand" the content, objects, scenes, and relationships within an image. This is like giving robots a pair of thinking eyes.'}),"\n",(0,s.jsx)(n.p,{children:'The core tool for this feature is **seewhat`. When a user issues a command like "see what\'s here," the system invokes this tool, triggering a series of background operations that ultimately provide the user with AI-generated analysis of the live image in natural language.'}),"\n",(0,s.jsx)(n.h3,{id:"12-implementation-principle-overview",children:"1.2 Implementation Principle Overview"}),"\n",(0,s.jsx)(n.p,{children:"The basic principle is to input two different types of information\u2014 image (visual information) and text (linguistic information) \u2014into a powerful multimodal large model (such as LLaVA)."}),"\n",(0,s.jsx)(n.p,{children:"Simply put, this involves highlighting the corresponding parts of the image with text, and then describing the highlighted parts with language ."}),"\n",(0,s.jsx)(n.h2,{id:"2-code-explanation",children:"2. Code Explanation"}),"\n",(0,s.jsx)(n.h3,{id:"key-code",children:"Key Code"}),"\n",(0,s.jsx)(n.h4,{id:"1-tools-layer-entry--largemodelutilstools_managerpy-",children:"1. Tools Layer Entry ( largemodel/utils/tools_manager.py )"}),"\n",(0,s.jsx)(n.p,{children:"The seewhat function in this file defines the execution flow of the tool."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'xxxxxxxxxx\n# From largemodel/utils/tools_manager.py\n\u200b\nclass\nToolsManager\n:\n# ...\n\u200b\ndef\nseewhat\n(\nself\n):\n"""\nCapture camera frame and analyze environment with AI model.\n\u6355\u83b7\u6444\u50cf\u5934\u753b\u9762\u5e76\u4f7f\u7528AI\u6a21\u578b\u5206\u6790\u73af\u5883\u3002\n:return: Dictionary with scene description and image path, or None if failed.\n"""\nself\n.\nnode\n.\nget_logger\n().\ninfo\n(\n"Executing seewhat() tool"\n)\nimage_path\n=\nself\n.\ncapture_frame\n()\nif\nimage_path\n:\n# Use isolated context for image analysis. / \u4f7f\u7528\u9694\u79bb\u7684\u4e0a\u4e0b\u6587\u8fdb\u884c\u56fe\u50cf\u5206\u6790\u3002\nanalysis_text\n=\nself\n.\n_get_actual_scene_description\n(\nimage_path\n)\n\u200b\n# Return structured data for the tool chain. / \u4e3a\u5de5\u5177\u94fe\u8fd4\u56de\u7ed3\u6784\u5316\u6570\u636e\u3002\nreturn\n{\n"description"\n:\nanalysis_text\n,\n"image_path"\n:\nimage_path\n}\nelse\n:\n# ... (Error handling)\nreturn\nNone\n\u200b\ndef\n_get_actual_scene_description\n(\nself\n,\nimage_path\n,\nmessage_context\n=\nNone\n):\n"""\nGet AI-generated scene description for captured image.\n\u83b7\u53d6\u6355\u83b7\u56fe\u50cf\u7684AI\u751f\u6210\u573a\u666f\u63cf\u8ff0\u3002\n:param image_path: Path to captured image file.\n:return: Plain text description of scene.\n"""\ntry\n:\n# ... (\u6784\u5efaPrompt)\n# Force use of a plain text system prompt with a clean, one-time context. / \u5f3a\u5236\u4f7f\u7528\u7eaf\u6587\u672c\u7cfb\u7edf\u63d0\u793a\u548c\u5e72\u51c0\u7684\u4e00\u6b21\u6027\u4e0a\u4e0b\u6587\u3002\nsimple_context\n= [{\n"role"\n:\n"system"\n,\n"content"\n:\n"You are an image description assistant. ..."\n}]\n\u200b\nresult\n=\nself\n.\nnode\n.\nmodel_client\n.\ninfer_with_image\n(\nimage_path\n,\nscene_prompt\n,\nmessage\n=\nsimple_context\n)\n# ... (\u5904\u7406\u7ed3\u679c)\nreturn\ndescription\nexcept\nException\nas\ne\n:\n# ...\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# From largemodel/utils/tools_manager.py\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"class\nToolsManager\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ...\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"def\nseewhat\n(\nself\n):\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"""\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Capture camera frame and analyze environment with AI model.\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u6355\u83b7\u6444\u50cf\u5934\u753b\u9762\u5e76\u4f7f\u7528AI\u6a21\u578b\u5206\u6790\u73af\u5883\u3002\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:":return: Dictionary with scene description and image path, or None if failed.\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"""\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'self\n.\nnode\n.\nget_logger\n().\ninfo\n(\n"Executing seewhat() tool"\n)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"image_path\n=\nself\n.\ncapture_frame\n()\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"if\nimage_path\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Use isolated context for image analysis. / \u4f7f\u7528\u9694\u79bb\u7684\u4e0a\u4e0b\u6587\u8fdb\u884c\u56fe\u50cf\u5206\u6790\u3002\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"analysis_text\n=\nself\n.\n_get_actual_scene_description\n(\nimage_path\n)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Return structured data for the tool chain. / \u4e3a\u5de5\u5177\u94fe\u8fd4\u56de\u7ed3\u6784\u5316\u6570\u636e\u3002\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"return\n{\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"description"\n:\nanalysis_text\n,\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"image_path"\n:\nimage_path\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"}\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"else\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (Error handling)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"return\nNone\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"def\n_get_actual_scene_description\n(\nself\n,\nimage_path\n,\nmessage_context\n=\nNone\n):\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"""\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Get AI-generated scene description for captured image.\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u83b7\u53d6\u6355\u83b7\u56fe\u50cf\u7684AI\u751f\u6210\u573a\u666f\u63cf\u8ff0\u3002\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:":param image_path: Path to captured image file.\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:":return: Plain text description of scene.\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"""\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"try\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (\u6784\u5efaPrompt)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Force use of a plain text system prompt with a clean, one-time context. / \u5f3a\u5236\u4f7f\u7528\u7eaf\u6587\u672c\u7cfb\u7edf\u63d0\u793a\u548c\u5e72\u51c0\u7684\u4e00\u6b21\u6027\u4e0a\u4e0b\u6587\u3002\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"simple_context\n= [{\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"role"\n:\n"system"\n,\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"content"\n:\n"You are an image description assistant. ..."\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"}]\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"result\n=\nself\n.\nnode\n.\nmodel_client\n.\ninfer_with_image\n(\nimage_path\n,\nscene_prompt\n,\nmessage\n=\nsimple_context\n)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (\u5904\u7406\u7ed3\u679c)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"return\ndescription\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"except\nException\nas\ne\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ...\n"})}),"\n",(0,s.jsx)(n.h4,{id:"2-model-interface-layer--largemodelutilslarge_model_interfacepy-",children:"2. Model interface layer ( largemodel/utils/large_model_interface.py )"}),"\n",(0,s.jsx)(n.p,{children:"The infer_with_image function in this file is the unified entry point for all image understanding tasks. It is responsible for calling the specific model implementation according to the configuration."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n# From largemodel/utils/large_model_interface.py\n\u200b\nclass\nmodel_interface\n:\n# ...\ndef\ninfer_with_image\n(\nself\n,\nimage_path\n,\ntext\n=\nNone\n,\nmessage\n=\nNone\n):\n\"\"\"Unified image inference interface. / \u7edf\u4e00\u7684\u56fe\u50cf\u63a8\u7406\u63a5\u53e3\u3002\"\"\"\n# ... (\u51c6\u5907\u6d88\u606f)\ntry\n:\n# \u6839\u636e self.llm_platform \u7684\u503c\uff0c\u51b3\u5b9a\u8c03\u7528\u54ea\u4e2a\u5177\u4f53\u5b9e\u73b0\nif\nself\n.\nllm_platform\n==\n'ollama'\n:\nresponse_content\n=\nself\n.\nollama_infer\n(\nself\n.\nmessages\n,\nimage_path\n=\nimage_path\n)\nelif\nself\n.\nllm_platform\n==\n'tongyi'\n:\n# ... \u8c03\u7528\u901a\u4e49\u6a21\u578b\u7684\u903b\u8f91\npass\n# ... (\u5176\u4ed6\u5e73\u53f0\u7684\u903b\u8f91)\n# ...\nreturn\n{\n'response'\n:\nresponse_content\n,\n'messages'\n:\nself\n.\nmessages\n.\ncopy\n()}\n\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# From largemodel/utils/large_model_interface.py\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"class\nmodel_interface\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ...\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"def\ninfer_with_image\n(\nself\n,\nimage_path\n,\ntext\n=\nNone\n,\nmessage\n=\nNone\n):\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"""Unified image inference interface. / \u7edf\u4e00\u7684\u56fe\u50cf\u63a8\u7406\u63a5\u53e3\u3002"""\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (\u51c6\u5907\u6d88\u606f)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"try\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# \u6839\u636e self.llm_platform \u7684\u503c\uff0c\u51b3\u5b9a\u8c03\u7528\u54ea\u4e2a\u5177\u4f53\u5b9e\u73b0\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"if\nself\n.\nllm_platform\n==\n'ollama'\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"response_content\n=\nself\n.\nollama_infer\n(\nself\n.\nmessages\n,\nimage_path\n=\nimage_path\n)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"elif\nself\n.\nllm_platform\n==\n'tongyi'\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... \u8c03\u7528\u901a\u4e49\u6a21\u578b\u7684\u903b\u8f91\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pass\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (\u5176\u4ed6\u5e73\u53f0\u7684\u903b\u8f91)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ...\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"return\n{\n'response'\n:\nresponse_content\n,\n'messages'\n:\nself\n.\nmessages\n.\ncopy\n()}\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.h3,{id:"code-analysis",children:"Code Analysis"}),"\n",(0,s.jsx)(n.p,{children:"This feature's implementation involves two main layers: the tool layer defines the business logic, and the model interface layer is responsible for communicating with the large language model. This layered design is key to achieving platform versatility."}),"\n",(0,s.jsx)(n.p,{children:'In summary, the seewhat tool\'s execution flow demonstrates a clear separation of responsibilities: ToolsManager defines the "what" (acquiring an image and requesting analysis), while model_interface defines the "how" (selecting the appropriate model platform based on the current configuration and interacting with it). This makes the tutorial\'s analysis universal, ensuring the core code logic remains consistent regardless of whether the user is online or offline.'}),"\n",(0,s.jsx)(n.h2,{id:"31-configuring-the-offline-large-model",children:"3.1 Configuring the Offline Large Model"}),"\n",(0,s.jsx)(n.h4,{id:"311-configuring-the-llm-platform--hemihexyaml-",children:"3.1.1 Configuring the LLM Platform ( HemiHex.yaml )"}),"\n",(0,s.jsx)(n.p,{children:"This file determines which large model platform the model_service node loads as its primary language model."}),"\n",(0,s.jsx)(n.p,{children:"Open the file in the terminal :"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\nvim\n~/yahboom_ws/src/largemodel/config/HemiHex.yaml\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"vim\n~/yahboom_ws/src/largemodel/config/HemiHex.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:"Modify/Confirm llm_platform :"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\nmodel_service\n:\n#\u6a21\u578b\u670d\u52a1\u5668\u8282\u70b9\u53c2\u6570 Model server node parameters\nros__parameters\n:\nlanguage\n:\n'en'\n#\u5927\u6a21\u578b\u63a5\u53e3\u8bed\u8a00 Large Model Interface Language\nuseolinetts\n:\nTrue\n#\u6587\u5b57\u6a21\u5f0f\u4e0b\u6b64\u9879\u65e0\u6548\uff0c\u53ef\u5ffd\u7565 This item is invalid in text mode and can be ignored\n\u200b\n# \u5927\u6a21\u578b\u914d\u7f6e Large model configuration\nllm_platform\n:\n'ollama'\n# \u5173\u952e: \u786e\u4fdd\u8fd9\u91cc\u662f 'ollama' Key: Make sure it's 'ollama'\nregional_setting\n:\n\"international\"\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"model_service\n:\n#\u6a21\u578b\u670d\u52a1\u5668\u8282\u70b9\u53c2\u6570 Model server node parameters\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros__parameters\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"language\n:\n'en'\n#\u5927\u6a21\u578b\u63a5\u53e3\u8bed\u8a00 Large Model Interface Language\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"useolinetts\n:\nTrue\n#\u6587\u5b57\u6a21\u5f0f\u4e0b\u6b64\u9879\u65e0\u6548\uff0c\u53ef\u5ffd\u7565 This item is invalid in text mode and can be ignored\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# \u5927\u6a21\u578b\u914d\u7f6e Large model configuration\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"llm_platform\n:\n'ollama'\n# \u5173\u952e: \u786e\u4fdd\u8fd9\u91cc\u662f 'ollama' Key: Make sure it's 'ollama'\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'regional_setting\n:\n"international"\n'})}),"\n",(0,s.jsx)(n.h4,{id:"312-configuring-the-model-interface--large_model_interfaceyaml-",children:"3.1.2 Configuring the Model Interface ( large_model_interface.yaml )"}),"\n",(0,s.jsx)(n.p,{children:"This file defines which vision model to use when the ollama platform is selected."}),"\n",(0,s.jsx)(n.p,{children:"1.Open the file in Terminal"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\nvim ~/yahboom_ws/src/largemodel/config/large_model_interface.yaml\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"vim ~/yahboom_ws/src/largemodel/config/large_model_interface.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:"2.Find the ollama related configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'xxxxxxxxxx\n#.....\n## \u79bb\u7ebf\u5927\u6a21\u578b (Offline Large Language Models)\n# Ollama Configuration\nollama_host:\n"http://localhost:11434"\n# Ollama server address\nollama_model:\n"llava"\n# Key: Change this to the multimodal model you downloaded, such as "llava"\n#.....\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"#.....\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"## \u79bb\u7ebf\u5927\u6a21\u578b (Offline Large Language Models)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Ollama Configuration\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'ollama_host:\n"http://localhost:11434"\n# Ollama server address\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'ollama_model:\n"llava"\n# Key: Change this to the multimodal model you downloaded, such as "llava"\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"#.....\n"})}),"\n",(0,s.jsx)(n.p,{children:"Note : Please make sure that the model specified in the configuration parameters (such as llava ) can handle multimodal input."}),"\n",(0,s.jsx)(n.h3,{id:"32-starting-and-testing-the-function",children:"3.2 Starting and Testing the Function"}),"\n",(0,s.jsx)(n.p,{children:"Note: Due to performance limitations, this example cannot be run on the Jetson Orin Nano 4GB. To experience this function, please refer to the corresponding section in [Online Large Model (Voice Interaction)]"}),"\n",(0,s.jsx)(n.p,{children:"Start the largemodel main program : Open a terminal and run the following command:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\nros2 launch largemodel largemodel_control.launch.py\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch largemodel largemodel_control.launch.py\n"})}),"\n",(0,s.jsx)(n.p,{children:'After successful initialization, say the wake-up word and begin asking questions like: "What do you see?" or "Describe your current environment."'}),"\n",(0,s.jsx)(n.p,{children:"Observe the results : In the first terminal where the main program is running, you will see log output showing that the system receives the text command, invokes the seewhat tool, and ultimately prints the text description generated by the LLaVA model. The speaker will also announce the generated results."}),"\n",(0,s.jsx)(n.h2,{id:"4-common-problems-and-solutions",children:"4. Common Problems and Solutions"}),"\n",(0,s.jsx)(n.h3,{id:"41-very-slow-responses",children:"4.1 Very Slow Responses"}),"\n",(0,s.jsx)(n.p,{children:"Problem : After asking a question, it takes a long time for the voice response to arrive. Solution : The inference cost of a multimodal model is much higher than that of a text-only model, so higher latency is normal."})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);