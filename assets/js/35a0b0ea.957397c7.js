"use strict";(globalThis.webpackChunkhemihex_docs=globalThis.webpackChunkhemihex_docs||[]).push([[2856],{28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var a=t(96540);const i={},l=a.createContext(i);function o(e){const n=a.useContext(l);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(l.Provider,{value:n},e.children)}},65218:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"HH-101/Online AI Model/Multimodal-Table-Scanning-Application","title":"Multimodal table scanning application","description":"1. Concept Introduction","source":"@site/docs/hh101/HH-101/12 - Online AI Model/04-Multimodal-Table-Scanning-Application.md","sourceDirName":"HH-101/12 - Online AI Model","slug":"/HH-101/Online AI Model/Multimodal-Table-Scanning-Application","permalink":"/hh-101/HH-101/Online AI Model/Multimodal-Table-Scanning-Application","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Multimodal table scanning application","sidebar_position":4},"sidebar":"hh101Sidebar","previous":{"title":"Multimodal visual localization application","permalink":"/hh-101/HH-101/Online AI Model/Multimodal-Visual-Localization-Application"},"next":{"title":"Multimodal autonomous proxy application","permalink":"/hh-101/HH-101/Online AI Model/Multimodal-Autonomous-Proxy-Application"}}');var i=t(74848),l=t(28453);const o={title:"Multimodal table scanning application",sidebar_position:4},r="4.Multimodal table scanning application",s={},c=[{value:"1. Concept Introduction",id:"1-concept-introduction",level:2},{value:"1.1 What is &quot;Multimodal Table Scanning&quot;?",id:"11-what-is-multimodal-table-scanning",level:3},{value:"1.2 Implementation Principles",id:"12-implementation-principles",level:3},{value:"2. Code Analysis",id:"2-code-analysis",level:2},{value:"Key Code",id:"key-code",level:3},{value:"1. Tools Layer Entry (largemodel/utils/tools_manager.py)",id:"1-tools-layer-entry-largemodelutilstools_managerpy",level:4},{value:"2. Model interface layer (largemodel/utils/large_model_interface.py)",id:"2-model-interface-layer-largemodelutilslarge_model_interfacepy",level:4},{value:"Code Analysis",id:"code-analysis",level:3},{value:"3. Practical Application",id:"3-practical-application",level:2},{value:"3.1 Configuring Online LLM",id:"31-configuring-online-llm",level:3},{value:"3.2 Starting and Testing the Function",id:"32-starting-and-testing-the-function",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,l.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"4multimodal-table-scanning-application",children:"4.Multimodal table scanning application"})}),"\n",(0,i.jsx)(n.h2,{id:"1-concept-introduction",children:"1. Concept Introduction"}),"\n",(0,i.jsx)(n.h3,{id:"11-what-is-multimodal-table-scanning",children:'1.1 What is "Multimodal Table Scanning"?'}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Multimodal table scanning"}),"is a technology that uses image processing and artificial intelligence to identify and extract table information from images or PDF documents. It not only focuses on visual table structure recognition but also integrates multimodal data such as text content and layout information to enhance table understanding.**Large Language Models (LLMs)**provide powerful semantic analysis capabilities to understand this extracted information. The two complement each other and enhance the intelligent level of document processing."]}),"\n",(0,i.jsx)(n.h3,{id:"12-implementation-principles",children:"1.2 Implementation Principles"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Table Detection and Content Recognition Utilizes computer vision technology to locate tables in documents and uses optical character recognition (OCR) technology to convert the text within the tables into an editable format. Utilizes deep learning methods to analyze the table structure (row and column division, cell merging, etc.) and generate a structured data representation."}),"\n",(0,i.jsx)(n.li,{children:"Multimodal Fusion Integrate visual information (such as table layout), text (OCR results), and any metadata (such as file type and source) to form a comprehensive view of the data. Use a specially designed multimodal model (such as LayoutLM) to simultaneously process these different types of data to more accurately understand the table content and its context."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"2-code-analysis",children:"2. Code Analysis"}),"\n",(0,i.jsx)(n.h3,{id:"key-code",children:"Key Code"}),"\n",(0,i.jsx)(n.h4,{id:"1-tools-layer-entry-largemodelutilstools_managerpy",children:"1. Tools Layer Entry (largemodel/utils/tools_manager.py)"}),"\n",(0,i.jsxs)(n.p,{children:["The",(0,i.jsx)(n.code,{children:"scan_table"}),"function in this file defines the tool's execution flow, specifically how it constructs a prompt that returns a Markdown-formatted result."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# From largemodel/utils/tools_manager.py\nclass ToolsManager:\n# ...\ndef scan_table(self, args):\n"""\nScan a table from an image and save the content as a Markdown file.\n\n:param args: Arguments containing the image path.\n:return: Dictionary with file path and content.\n"""\nself.node.get_logger().info(f"Executing scan_table() tool with args: {args}")\ntry:\nimage_path = args.get("image_path")\n# ... (path checking and fallback)\n\n# Construct a prompt asking the large model to recognize the table and return it in Markdown format.\n\nif self.node.language == \'zh\':\nprompt = "Please carefully analyze this image, identify the table, and return its content in Markdown format.  # translated from Chinese"\nelse:\nprompt = "Please carefully analyze this image, identify the table within it, and return its content in Markdown format."\n\nresult = self.node.model_client.infer_with_image(image_path, prompt)\n\n# ... (Extract Markdown text from the result)\n\n# Save the recognized content to a Markdown file.\nmd_file_path = os.path.join(self.node.pkg_path, "resources_file", "scanned_tables", f"table_{timestamp}.md")\nwith open(md_file_path, \'w\', encoding=\'utf-8\') as f:\nf.write(table_content)\n\nreturn {\n"file_path": md_file_path,\n"table_content": table_content\n}\n# ... (error handling)\n'})}),"\n",(0,i.jsx)(n.h4,{id:"2-model-interface-layer-largemodelutilslarge_model_interfacepy",children:"2. Model interface layer (largemodel/utils/large_model_interface.py)"}),"\n",(0,i.jsxs)(n.p,{children:["The",(0,i.jsx)(n.code,{children:"infer_with_image"}),"function in this file is the unified entry point for all image-related tasks."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# From largemodel/utils/large_model_interface.py\n\nclass model_interface:\n# ...\ndef infer_with_image(self, image_path, text=None, message=None):\n\"\"\"Unified image inference interface. \"\"\"\n# ... (prepare message)\ntry:\n# Determine which specific implementation to call based on the value of self.llm_platform\nif self.llm_platform == 'ollama':\nresponse_content = self.ollama_infer(self.messages, image_path=image_path)\nelif self.llm_platform == 'tongyi':\n# ... Calling the logic of the Tongyi model\npass\n# ... (Logic for other platforms)\n# ...\nreturn {'response': response_content, 'messages': self.messages.copy()}\n"})}),"\n",(0,i.jsx)(n.h3,{id:"code-analysis",children:"Code Analysis"}),"\n",(0,i.jsxs)(n.p,{children:["The table scanning function is a typical application for converting unstructured image data into structured text data. Its core technology remains",(0,i.jsx)(n.strong,{children:"guiding model behavior through prompt engineering"}),"."]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Tools Layer ( tools_manager.py ) : The scan_table function is the business process controller for this function. It receives an image containing a table as input. The key operation of this function is building a targeted prompt . This prompt directly instructs the large model to perform two tasks: 1. Recognize the table in the image. 2. Return the recognized content in Markdown format. This mandatory output format is key to achieving unstructured-to-structured conversion. After constructing the prompt, it calls the infer_with_image method of the model interface layer, passing the image and the formatting instructions. After receiving the Markdown text returned from the model interface layer, it performs a file operation: writing the text content to a new .md file. Finally, it returns structured data containing the new file path and table contents."}),"\n",(0,i.jsx)(n.li,{children:'Model Interface Layer ( large_model_interface.py ) : The infer_with_image function continues to serve as the unified "dispatching center." It receives the image and prompt from scan_table and dispatches the task to the correct backend model implementation based on the current system configuration ( self.llm_platform ). Regardless of the backend model, this layer handles the communication details with the specific platform, ensuring that the image and text data are sent correctly, and then returns the plain text (in this case, Markdown-formatted text) returned by the model to the tooling layer.'}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"In summary, the general workflow for table scanning is: ToolsManager receives an image and constructs a command to convert the table in this image to Markdown. ToolsManager calls the model interface. ModelInterface packages the image and the command and sends it to the corresponding model platform according to the configuration. The model returns Markdown-formatted text. ModelInterface returns the text to ToolsManager. ToolsManager saves the text as a .md file and returns the result. This workflow demonstrates how to leverage the formatting capabilities of a large model as a powerful OCR (Optical Character Recognition) and data structuring tool."}),"\n",(0,i.jsx)(n.h2,{id:"3-practical-application",children:"3. Practical Application"}),"\n",(0,i.jsx)(n.h3,{id:"31-configuring-online-llm",children:"3.1 Configuring Online LLM"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"First, obtain the API key from any of the platforms described in the previous tutorials."}),"\n",(0,i.jsx)(n.li,{children:"Next, update the key in the configuration file. Open the model interface configuration file. large_model_interface.yaml : xxxxxxxxxx vim ~/hemihex_ws/src/largemodel/config/large_model_interface.yaml"}),"\n",(0,i.jsx)(n.li,{children:'Enter your API Key : Find the corresponding section and paste the API Key you just copied. This example uses the Tongyi Qianwen configuration. xxxxxxxxxx # large_model_interface.yaml  ## Thousand Questions on Tongyi qianwen_api_key : "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" # Paste your Key qianwen_model : "qwen-vl-max-latest" # You can choose the model as needed, such as qwen-turbo, qwen-plus'}),"\n",(0,i.jsx)(n.li,{children:"Open the main configuration file HemiHex.yaml : xxxxxxxxxx vim ~/hemihex_ws/src/largemodel/config/HemiHex.yaml"}),"\n",(0,i.jsx)(n.li,{children:"Select the online platform you want to use : Change the llm_platform parameter to the platform name you want to use. xxxxxxxxxx # HemiHex.yaml  model_service : ros__parameters : # ... llm_platform : 'tongyi' #Optional platforms: 'ollama', 'tongyi', 'spark', 'qianfan', 'openrouter'"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"32-starting-and-testing-the-function",children:"3.2 Starting and Testing the Function"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Prepare a table image file :"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Place a table image file to be tested in the following directory:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"/home/jetson/hemihex_ws/src/largemodel/resources_file/scan_table"})}),"\n",(0,i.jsxs)(n.p,{children:["Then name the image",(0,i.jsx)(n.code,{children:"test_table.jpg"})]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Start the largemodel main program :"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Open a terminal and run the following command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"ros2 launch largemodel largemodel_control.launch.py text_chat_mode:=true\n"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Send text command : Open another terminal and run the following command: xxxxxxxxxx ros2 run text_chat text_chat"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:'Then start typing: "Analyze the table."'}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Observe the results : In the first terminal running the main program, you will see log output indicating that the system received the command, called the scan_table tool, and completed the scan, saving the scanned information to a file."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This file can be found in the ~/hemihex_ws/src/largemodel/resources_file/scan_table directory."})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);