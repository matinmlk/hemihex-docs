"use strict";(globalThis.webpackChunkhemihex_docs=globalThis.webpackChunkhemihex_docs||[]).push([[576],{28453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>r});var l=a(96540);const i={},t=l.createContext(i);function o(e){const n=l.useContext(t);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),l.createElement(t.Provider,{value:n},e.children)}},92058:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>s,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>d});const l=JSON.parse('{"id":"HH-101/Offline AI Model/multimodal-autonomous-agent-application-11-offlineaimodel-11-16","title":"Multimodal Autonomous Agent Application","description":"1. Concept Introduction","source":"@site/docs/hh101/HH-101/11 - Offline AI Model/16-multimodal-autonomous-agent-application-11-offlineaimodel-11-16.md","sourceDirName":"HH-101/11 - Offline AI Model","slug":"/HH-101/Offline AI Model/multimodal-autonomous-agent-application-11-offlineaimodel-11-16","permalink":"/hh-101/HH-101/Offline AI Model/multimodal-autonomous-agent-application-11-offlineaimodel-11-16","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"title":"Multimodal Autonomous Agent Application","sidebar_position":0},"sidebar":"hh101Sidebar","previous":{"title":"Multimodal Table Scanning Application","permalink":"/hh-101/HH-101/Offline AI Model/multimodal-table-scanning-application-11-offlineaimodel-11-15"},"next":{"title":"Offline Speech to Text (ASR)","permalink":"/hh-101/HH-101/Offline AI Model/offline-speech-to-text-asr-11-offlineaimodel-11-18"}}');var i=a(74848),t=a(28453);const o={title:"Multimodal Autonomous Agent Application",sidebar_position:0},r="Multimodal autonomous agent application",s={},d=[{value:"1. Concept Introduction",id:"1-concept-introduction",level:2},{value:"1.1 What is an &quot;Autonomous Agent&quot;?",id:"11-what-is-an-autonomous-agent",level:3},{value:"1.2 Implementation Principles",id:"12-implementation-principles",level:3},{value:"2. Code Analysis",id:"2-code-analysis",level:2},{value:"Key Code",id:"key-code",level:3},{value:"1. Agent Core Workflow ( largemodel/utils/ai_agent.py )",id:"1-agent-core-workflow--largemodelutilsai_agentpy-",level:4},{value:"2. Mission planning and LLM interaction ( largemodel/utils/ai_agent.py )",id:"2-mission-planning-and-llm-interaction--largemodelutilsai_agentpy-",level:4},{value:"3. Parameter processing and data flow implementation ( largemodel/utils/ai_agent.py )",id:"3-parameter-processing-and-data-flow-implementation--largemodelutilsai_agentpy-",level:4},{value:"Code Analysis",id:"code-analysis",level:3},{value:"3. Practical Operations",id:"3-practical-operations",level:2},{value:"3.1 Configuring the Offline Large Model",id:"31-configuring-the-offline-large-model",level:3},{value:"3.1.1 Configuring the LLM Platform ( HemiHex.yaml )",id:"311-configuring-the-llm-platform--hemihexyaml-",level:4},{value:"3.1.2 Configuration model interface ( large_model_interface.yaml )",id:"312-configuration-model-interface--large_model_interfaceyaml-",level:4},{value:"3.2 Starting and Testing the Functionality (Text Input Mode)",id:"32-starting-and-testing-the-functionality-text-input-mode",level:3},{value:"4. Common Problems and Solutions",id:"4-common-problems-and-solutions",level:2},{value:"4.1 Abnormal Agent Behavior",id:"41-abnormal-agent-behavior",level:3},{value:"Issue 1: The agent is stuck in an infinite loop or repeatedly executing the same tool.",id:"issue-1-the-agent-is-stuck-in-an-infinite-loop-or-repeatedly-executing-the-same-tool",level:4},{value:"4.2 Tool Invocation Failure",id:"42-tool-invocation-failure",level:3},{value:"Issue 2: The agent correctly planned the action, but the tool execution failed.",id:"issue-2-the-agent-correctly-planned-the-action-but-the-tool-execution-failed",level:4}];function c(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",p:"p",pre:"pre",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"multimodal-autonomous-agent-application",children:"Multimodal autonomous agent application"})}),"\n",(0,i.jsx)(n.h2,{id:"1-concept-introduction",children:"1. Concept Introduction"}),"\n",(0,i.jsx)(n.h3,{id:"11-what-is-an-autonomous-agent",children:'1.1 What is an "Autonomous Agent"?'}),"\n",(0,i.jsx)(n.p,{children:"In the largemodel project, multimodal autonomous agents represent the most advanced form of intelligence. Rather than simply responding to a user's command, they are capable of autonomously thinking, planning, and continuously invoking multiple tools to achieve a complex goal ."}),"\n",(0,i.jsx)(n.p,{children:"The core of this functionality is the **agent_call ** tool or its underlying **ToolChainManager . When a user issues a complex request that cannot be accomplished with a single tool call, the autonomous agent is activated."}),"\n",(0,i.jsx)(n.h3,{id:"12-implementation-principles",children:"1.2 Implementation Principles"}),"\n",(0,i.jsx)(n.p,{children:'The autonomous agent implementation in largemodel follows the industry-leading ReAct (Reason + Act) paradigm. Its core concept is to mimic the human problem-solving process, cycling between "thinking" and "acting."'}),"\n",(0,i.jsx)(n.p,{children:"This think -> act -> observe cycle continues until the initial goal is achieved, at which point the agent generates and outputs the final answer."}),"\n",(0,i.jsx)(n.h2,{id:"2-code-analysis",children:"2. Code Analysis"}),"\n",(0,i.jsx)(n.h3,{id:"key-code",children:"Key Code"}),"\n",(0,i.jsx)(n.h4,{id:"1-agent-core-workflow--largemodelutilsai_agentpy-",children:"1. Agent Core Workflow ( largemodel/utils/ai_agent.py )"}),"\n",(0,i.jsx)(n.p,{children:'The _execute_agent_workflow function is the agent\'s main execution loop, defining the core "plan -> execute" process.\r\n[TODO14]'}),"\n",(0,i.jsx)(n.h4,{id:"2-mission-planning-and-llm-interaction--largemodelutilsai_agentpy-",children:"2. Mission planning and LLM interaction ( largemodel/utils/ai_agent.py )"}),"\n",(0,i.jsx)(n.p,{children:"The core of the _plan_task function is to build a sophisticated prompt and use the reasoning ability of the large model to generate a structured execution plan."}),"\n",(0,i.jsx)(n.p,{children:"[TODO]"}),"\n",(0,i.jsx)(n.h4,{id:"3-parameter-processing-and-data-flow-implementation--largemodelutilsai_agentpy-",children:"3. Parameter processing and data flow implementation ( largemodel/utils/ai_agent.py )"}),"\n",(0,i.jsx)(n.p,{children:"The _process_step_parameters function is responsible for parsing placeholders and implementing data flow between steps.\r\n[TODO]"}),"\n",(0,i.jsx)(n.h3,{id:"code-analysis",children:"Code Analysis"}),"\n",(0,i.jsx)(n.p,{children:'The AI \u200b\u200bAgent is the "brain" of the system, translating high-level, sometimes ambiguous, tasks posed by the user into a precise, ordered series of tool calls. Its implementation is independent of any specific model platform and built on a general, extensible architecture.'}),"\n",(0,i.jsx)(n.p,{children:"In summary, the general implementation of the AI \u200b\u200bAgent demonstrates an advanced software architecture: rather than solving a problem directly, it builds a framework that enables an external, general-purpose reasoning engine (a large model) to solve the problem. Through two core mechanisms, dynamic programming and data flow management, the Agent orchestrates a series of independent tools into complex workflows capable of completing advanced tasks."}),"\n",(0,i.jsx)(n.h2,{id:"3-practical-operations",children:"3. Practical Operations"}),"\n",(0,i.jsx)(n.h3,{id:"31-configuring-the-offline-large-model",children:"3.1 Configuring the Offline Large Model"}),"\n",(0,i.jsx)(n.h4,{id:"311-configuring-the-llm-platform--hemihexyaml-",children:"3.1.1 Configuring the LLM Platform ( HemiHex.yaml )"}),"\n",(0,i.jsx)(n.p,{children:"This file determines which large model platform the model_service node loads as its primary language model."}),"\n",(0,i.jsx)(n.p,{children:"Open the file in the terminal :"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"vim\r\n~/yahboom_ws/src/largemodel/config/HemiHex.yaml\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"vim\r\n~/yahboom_ws/src/largemodel/config/HemiHex.yaml\n"})}),"\n",(0,i.jsx)(n.p,{children:"Modify/confirm llm_platform :"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"model_service\r\n:\r\n#\u6a21\u578b\u670d\u52a1\u5668\u8282\u70b9\u53c2\u6570 Model server node parameters\r\nros__parameters\r\n:\r\nlanguage\r\n:\r\n'en'\r\n#\u5927\u6a21\u578b\u63a5\u53e3\u8bed\u8a00 Large Model Interface Language\r\nuseolinetts\r\n:\r\nFalse\r\n#\u6587\u5b57\u6a21\u5f0f\u4e0b\u6b64\u9879\u65e0\u6548\uff0c\u53ef\u5ffd\u7565 This item is invalid in text mode and can be ignored\r\n\u200b\r\n# \u5927\u6a21\u578b\u914d\u7f6e Large model configuration\r\nllm_platform\r\n:\r\n'ollama'\r\n# \u5173\u952e: \u786e\u4fdd\u8fd9\u91cc\u662f 'ollama' Key: Make sure it's 'ollama'\r\nregional_setting\r\n:\r\n\"international\"\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"model_service\r\n:\r\n#\u6a21\u578b\u670d\u52a1\u5668\u8282\u70b9\u53c2\u6570 Model server node parameters\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros__parameters\r\n:\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"language\r\n:\r\n'en'\r\n#\u5927\u6a21\u578b\u63a5\u53e3\u8bed\u8a00 Large Model Interface Language\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"useolinetts\r\n:\r\nFalse\r\n#\u6587\u5b57\u6a21\u5f0f\u4e0b\u6b64\u9879\u65e0\u6548\uff0c\u53ef\u5ffd\u7565 This item is invalid in text mode and can be ignored\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# \u5927\u6a21\u578b\u914d\u7f6e Large model configuration\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"llm_platform\r\n:\r\n'ollama'\r\n# \u5173\u952e: \u786e\u4fdd\u8fd9\u91cc\u662f 'ollama' Key: Make sure it's 'ollama'\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'regional_setting\r\n:\r\n"international"\n'})}),"\n",(0,i.jsx)(n.h4,{id:"312-configuration-model-interface--large_model_interfaceyaml-",children:"3.1.2 Configuration model interface ( large_model_interface.yaml )"}),"\n",(0,i.jsx)(n.p,{children:"This file defines which visual model to use when the platform is selected as ollama ."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\r\nvim ~/yahboom_ws/src/largemodel/config/large_model_interface.yaml\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"vim ~/yahboom_ws/src/largemodel/config/large_model_interface.yaml\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'#.....\r\n## \u79bb\u7ebf\u5927\u6a21\u578b (Offline Large Language Models)\r\n# Ollama Configuration\r\nollama_host:\r\n"http://localhost:11434"\r\n# Ollama server address\r\nollama_model:\r\n"llava"\r\n# Key: Change this to the multimodal model you downloaded, such as "llava"\r\n#.....\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"#.....\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"## \u79bb\u7ebf\u5927\u6a21\u578b (Offline Large Language Models)\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Ollama Configuration\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'ollama_host:\r\n"http://localhost:11434"\r\n# Ollama server address\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'ollama_model:\r\n"llava"\r\n# Key: Change this to the multimodal model you downloaded, such as "llava"\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"#.....\n"})}),"\n",(0,i.jsx)(n.p,{children:"Note : Please ensure that the model specified in the configuration parameters (e.g., llava ) can handle multimodal input."}),"\n",(0,i.jsx)(n.h3,{id:"32-starting-and-testing-the-functionality-text-input-mode",children:"3.2 Starting and Testing the Functionality (Text Input Mode)"}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsx)(n.p,{children:'Due to performance limitations, the performance of the Jetson Orin Nano 4GB is limited. To experience this feature, please refer to the "Online Large Model (Text Interaction)" section.'})}),"\n",(0,i.jsx)(n.p,{children:"Start the largemodel main program (text mode): Open a terminal and run the following command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 launch largemodel largemodel_control.launch.py text_chat_mode:\r\n=\r\ntrue\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 launch largemodel largemodel_control.launch.py text_chat_mode:\r\n=\r\ntrue\n"})}),"\n",(0,i.jsx)(n.p,{children:"Send text command : Open another terminal and run the following command:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\r\nros2 run text_chat text_chat\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 run text_chat text_chat\n"})}),"\n",(0,i.jsx)(n.p,{children:'Then start entering text: "Based on the current environment, save the generated environment description as a txt document."'}),"\n",(0,i.jsx)(n.p,{children:"Observe the Results :"}),"\n",(0,i.jsx)(n.p,{children:"In the first terminal running the main program, you will see log output indicating that the system receives the text command, invokes the aiagent tool, and then provides a prompt to LLM. LLM will analyze the detailed steps of the tool invocation. For example, in this question, the seewhat tool will be invoked to capture the image, which will then be parsed by LLM. The parsed text will be saved in the ~/yahboom_ws/src/largemodel/resources_file/documents folder."}),"\n",(0,i.jsx)(n.h2,{id:"4-common-problems-and-solutions",children:"4. Common Problems and Solutions"}),"\n",(0,i.jsx)(n.h3,{id:"41-abnormal-agent-behavior",children:"4.1 Abnormal Agent Behavior"}),"\n",(0,i.jsx)(n.h4,{id:"issue-1-the-agent-is-stuck-in-an-infinite-loop-or-repeatedly-executing-the-same-tool",children:"Issue 1: The agent is stuck in an infinite loop or repeatedly executing the same tool."}),"\n",(0,i.jsx)(n.p,{children:"Solution :"}),"\n",(0,i.jsx)(n.h3,{id:"42-tool-invocation-failure",children:"4.2 Tool Invocation Failure"}),"\n",(0,i.jsx)(n.h4,{id:"issue-2-the-agent-correctly-planned-the-action-but-the-tool-execution-failed",children:"Issue 2: The agent correctly planned the action, but the tool execution failed."}),"\n",(0,i.jsx)(n.p,{children:"Solution :"})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);