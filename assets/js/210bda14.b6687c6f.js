"use strict";(globalThis.webpackChunkhemihex_docs=globalThis.webpackChunkhemihex_docs||[]).push([[3708],{28453:(e,n,a)=>{a.d(n,{R:()=>r,x:()=>o});var l=a(96540);const s={},i=l.createContext(s);function r(e){const n=l.useContext(i);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),l.createElement(i.Provider,{value:n},e.children)}},84795:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>t,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>c});const l=JSON.parse('{"id":"HH-101/Offline AI Model/multimodal-visual-understanding-11-offlineaimodel-11-11","title":"Multimodal Visual Understanding Application","description":"1. Concept Introduction","source":"@site/docs/hh101/HH-101/11- Offline AI Model/11-multimodal-visual-understanding-11-offlineaimodel-11-11.md","sourceDirName":"HH-101/11- Offline AI Model","slug":"/HH-101/Offline AI Model/multimodal-visual-understanding-11-offlineaimodel-11-11","permalink":"/hh-101/HH-101/Offline AI Model/multimodal-visual-understanding-11-offlineaimodel-11-11","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"title":"Multimodal Visual Understanding Application","sidebar_position":0},"sidebar":"hh101Sidebar","previous":{"title":"MiniCPM-V","permalink":"/hh-101/HH-101/Offline AI Model/minicpm-v-11-offlineaimodel-11-10"},"next":{"title":"Multimodal Text Image Application","permalink":"/hh-101/HH-101/Offline AI Model/multimodal-text-image-application-11-offlineaimodel-11-12"}}');var s=a(74848),i=a(28453);const r={title:"Multimodal Visual Understanding Application",sidebar_position:0},o="Multimodal visual understand application",t={},c=[{value:"1. Concept Introduction",id:"1-concept-introduction",level:2},{value:"1.1 What is &quot;Visual Understanding&quot;?",id:"11-what-is-visual-understanding",level:3},{value:"1.2 Implementation Principle Overview",id:"12-implementation-principle-overview",level:3},{value:"2. Code Analysis",id:"2-code-analysis",level:2},{value:"Key Code",id:"key-code",level:3},{value:"1. Tool Layer Entry ( largemodel/utils/tools_manager.py )",id:"1-tool-layer-entry--largemodelutilstools_managerpy-",level:4},{value:"2. Model Interface Layer ( largemodel/utils/large_model_interface.py )",id:"2-model-interface-layer--largemodelutilslarge_model_interfacepy-",level:4},{value:"Code Analysis",id:"code-analysis",level:3},{value:"3.1 Configuring the Offline Large Model",id:"31-configuring-the-offline-large-model",level:2},{value:"3.1.1 Configuring the LLM Platform (HemiHex.yaml)",id:"311-configuring-the-llm-platform-hemihexyaml",level:4},{value:"3.1.2 Configuring the Model Interface ( large_model_interface.yaml )",id:"312-configuring-the-model-interface--large_model_interfaceyaml-",level:4},{value:"3.2 Starting and Testing the Function (Text Input Mode)",id:"32-starting-and-testing-the-function-text-input-mode",level:3},{value:"4. Common Problems and Solutions",id:"4-common-problems-and-solutions",level:2},{value:"Problem 1: The log displays &quot;Failed to call ollama vision model&quot; or the connection is refused.",id:"problem-1-the-log-displays-failed-to-call-ollama-vision-model-or-the-connection-is-refused",level:4},{value:"Problem 2: The seewhat tool returns &quot;Unable to open camera&quot; or fails to capture.",id:"problem-2-the-seewhat-tool-returns-unable-to-open-camera-or-fails-to-capture",level:4}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"multimodal-visual-understand-application",children:"Multimodal visual understand application"})}),"\n",(0,s.jsx)(n.h2,{id:"1-concept-introduction",children:"1. Concept Introduction"}),"\n",(0,s.jsx)(n.h3,{id:"11-what-is-visual-understanding",children:'1.1 What is "Visual Understanding"?'}),"\n",(0,s.jsx)(n.p,{children:'In the largemodel project, the multimodal visual understanding feature enables robots to go beyond simply "seeing" a matrix of pixels and truly "understand" the content, objects, scenes, and relationships within an image. This is like giving robots a pair of thinking eyes.'}),"\n",(0,s.jsx)(n.p,{children:'The core tool for this feature is **seewhat`. When a user issues a command like "see what\'s here," the system invokes this tool, triggering a series of background operations that ultimately provide the user with AI-generated analysis of the live image in natural language.'}),"\n",(0,s.jsx)(n.h3,{id:"12-implementation-principle-overview",children:"1.2 Implementation Principle Overview"}),"\n",(0,s.jsx)(n.p,{children:"The basic principle is to input two different types of information\u2014 image (visual information) and text (linguistic information) \u2014into a powerful multimodal large model (such as LLaVA)."}),"\n",(0,s.jsx)(n.p,{children:"Simply put, this involves highlighting the corresponding parts of the image with text, and then describing the highlighted parts with language ."}),"\n",(0,s.jsx)(n.h2,{id:"2-code-analysis",children:"2. Code Analysis"}),"\n",(0,s.jsx)(n.h3,{id:"key-code",children:"Key Code"}),"\n",(0,s.jsx)(n.h4,{id:"1-tool-layer-entry--largemodelutilstools_managerpy-",children:"1. Tool Layer Entry ( largemodel/utils/tools_manager.py )"}),"\n",(0,s.jsx)(n.p,{children:"The seewhat function in this file defines the tool's execution flow."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'\u200b\nx\n# From largemodel/utils/tools_manager.py\n\u200b\nclass\nToolsManager\n:\n# ...\n\u200b\ndef\nseewhat\n(\nself\n):\n"""\nCapture camera frame and analyze environment with AI model.\n\u6355\u83b7\u6444\u50cf\u5934\u753b\u9762\u5e76\u4f7f\u7528AI\u6a21\u578b\u5206\u6790\u73af\u5883\u3002\n:return: Dictionary with scene description and image path, or None if failed.\n"""\nself\n.\nnode\n.\nget_logger\n().\ninfo\n(\n"Executing seewhat() tool"\n)\nimage_path\n=\nself\n.\ncapture_frame\n()\nif\nimage_path\n:\n# Use isolated context for image analysis. / \u4f7f\u7528\u9694\u79bb\u7684\u4e0a\u4e0b\u6587\u8fdb\u884c\u56fe\u50cf\u5206\u6790\u3002\nanalysis_text\n=\nself\n.\n_get_actual_scene_description\n(\nimage_path\n)\n\u200b\n# Return structured data for the tool chain. / \u4e3a\u5de5\u5177\u94fe\u8fd4\u56de\u7ed3\u6784\u5316\u6570\u636e\u3002\nreturn\n{\n"description"\n:\nanalysis_text\n,\n"image_path"\n:\nimage_path\n}\nelse\n:\n# ... (Error handling)\nreturn\nNone\n\u200b\ndef\n_get_actual_scene_description\n(\nself\n,\nimage_path\n,\nmessage_context\n=\nNone\n):\n"""\nGet AI-generated scene description for captured image.\n\u83b7\u53d6\u6355\u83b7\u56fe\u50cf\u7684AI\u751f\u6210\u573a\u666f\u63cf\u8ff0\u3002\n:param image_path: Path to captured image file.\n:return: Plain text description of scene.\n"""\ntry\n:\n# ... (\u6784\u5efaPrompt)\nresult\n=\nself\n.\nnode\n.\nmodel_client\n.\ninfer_with_image\n(\nimage_path\n,\nscene_prompt\n,\nmessage\n=\nsimple_context\n)\n# ... (\u5904\u7406\u7ed3\u679c)\nreturn\ndescription\nexcept\nException\nas\ne\n:\n# ...\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# From largemodel/utils/tools_manager.py\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"class\nToolsManager\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ...\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"def\nseewhat\n(\nself\n):\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"""\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Capture camera frame and analyze environment with AI model.\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u6355\u83b7\u6444\u50cf\u5934\u753b\u9762\u5e76\u4f7f\u7528AI\u6a21\u578b\u5206\u6790\u73af\u5883\u3002\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:":return: Dictionary with scene description and image path, or None if failed.\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"""\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'self\n.\nnode\n.\nget_logger\n().\ninfo\n(\n"Executing seewhat() tool"\n)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"image_path\n=\nself\n.\ncapture_frame\n()\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"if\nimage_path\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Use isolated context for image analysis. / \u4f7f\u7528\u9694\u79bb\u7684\u4e0a\u4e0b\u6587\u8fdb\u884c\u56fe\u50cf\u5206\u6790\u3002\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"analysis_text\n=\nself\n.\n_get_actual_scene_description\n(\nimage_path\n)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Return structured data for the tool chain. / \u4e3a\u5de5\u5177\u94fe\u8fd4\u56de\u7ed3\u6784\u5316\u6570\u636e\u3002\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"return\n{\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"description"\n:\nanalysis_text\n,\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"image_path"\n:\nimage_path\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"}\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"else\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (Error handling)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"return\nNone\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"def\n_get_actual_scene_description\n(\nself\n,\nimage_path\n,\nmessage_context\n=\nNone\n):\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"""\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Get AI-generated scene description for captured image.\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u83b7\u53d6\u6355\u83b7\u56fe\u50cf\u7684AI\u751f\u6210\u573a\u666f\u63cf\u8ff0\u3002\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:":param image_path: Path to captured image file.\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:":return: Plain text description of scene.\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"""\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"try\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (\u6784\u5efaPrompt)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"result\n=\nself\n.\nnode\n.\nmodel_client\n.\ninfer_with_image\n(\nimage_path\n,\nscene_prompt\n,\nmessage\n=\nsimple_context\n)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (\u5904\u7406\u7ed3\u679c)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"return\ndescription\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"except\nException\nas\ne\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ...\n"})}),"\n",(0,s.jsx)(n.h4,{id:"2-model-interface-layer--largemodelutilslarge_model_interfacepy-",children:"2. Model Interface Layer ( largemodel/utils/large_model_interface.py )"}),"\n",(0,s.jsx)(n.p,{children:"The infer_with_image function in this file is the unified entry point for all image understanding tasks. It is responsible for calling the specific model implementation based on the configuration."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n# From largemodel/utils/large_model_interface.py\n\u200b\nclass\nmodel_interface\n:\n# ...\ndef\ninfer_with_image\n(\nself\n,\nimage_path\n,\ntext\n=\nNone\n,\nmessage\n=\nNone\n):\n\"\"\"Unified image inference interface. / \u7edf\u4e00\u7684\u56fe\u50cf\u63a8\u7406\u63a5\u53e3\u3002\"\"\"\n# ... (\u51c6\u5907\u6d88\u606f)\ntry\n:\n# \u6839\u636e self.llm_platform \u7684\u503c\uff0c\u51b3\u5b9a\u8c03\u7528\u54ea\u4e2a\u5177\u4f53\u5b9e\u73b0  Determine which specific implementation to call based on the value of self.llm_platform\nif\nself\n.\nllm_platform\n==\n'ollama'\n:\nresponse_content\n=\nself\n.\nollama_infer\n(\nself\n.\nmessages\n,\nimage_path\n=\nimage_path\n)\nelif\nself\n.\nllm_platform\n==\n'tongyi'\n:\n# ... Logic for calling the Tongyi model\npass\n# ... (\u5176\u4ed6\u5e73\u53f0\u7684\u903b\u8f91)\n# ...\nreturn\n{\n'response'\n:\nresponse_content\n,\n'messages'\n:\nself\n.\nmessages\n.\ncopy\n()}\n\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# From largemodel/utils/large_model_interface.py\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"class\nmodel_interface\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ...\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"def\ninfer_with_image\n(\nself\n,\nimage_path\n,\ntext\n=\nNone\n,\nmessage\n=\nNone\n):\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"""Unified image inference interface. / \u7edf\u4e00\u7684\u56fe\u50cf\u63a8\u7406\u63a5\u53e3\u3002"""\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (\u51c6\u5907\u6d88\u606f)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"try\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# \u6839\u636e self.llm_platform \u7684\u503c\uff0c\u51b3\u5b9a\u8c03\u7528\u54ea\u4e2a\u5177\u4f53\u5b9e\u73b0  Determine which specific implementation to call based on the value of self.llm_platform\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"if\nself\n.\nllm_platform\n==\n'ollama'\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"response_content\n=\nself\n.\nollama_infer\n(\nself\n.\nmessages\n,\nimage_path\n=\nimage_path\n)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"elif\nself\n.\nllm_platform\n==\n'tongyi'\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... Logic for calling the Tongyi model\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pass\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (\u5176\u4ed6\u5e73\u53f0\u7684\u903b\u8f91)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ...\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"return\n{\n'response'\n:\nresponse_content\n,\n'messages'\n:\nself\n.\nmessages\n.\ncopy\n()}\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.h3,{id:"code-analysis",children:"Code Analysis"}),"\n",(0,s.jsx)(n.p,{children:"This feature's implementation involves two primary layers: the tool layer defines the business logic, and the model interface layer is responsible for communicating with the large language model. This layered design is key to achieving platform versatility."}),"\n",(0,s.jsx)(n.p,{children:"Tool layer ( tools_manager.py ) :"}),"\n",(0,s.jsx)(n.p,{children:"Model interface layer ( large_model_interface.py ) :"}),"\n",(0,s.jsx)(n.p,{children:'In summary, the execution flow of the seewhat tool embodies a clear separation of responsibilities: ToolsManager is responsible for defining "what to do" (obtaining images and requesting analysis), while model_interface is responsible for defining "how to do it" (selecting the appropriate model platform based on the current configuration and interacting with it). This makes the tutorial\'s interpretation universal; the core code logic remains consistent regardless of whether the user is in online or offline mode.'}),"\n",(0,s.jsx)(n.h2,{id:"31-configuring-the-offline-large-model",children:"3.1 Configuring the Offline Large Model"}),"\n",(0,s.jsx)(n.h4,{id:"311-configuring-the-llm-platform-hemihexyaml",children:"3.1.1 Configuring the LLM Platform (HemiHex.yaml)"}),"\n",(0,s.jsx)(n.p,{children:"This file determines which large model platform the model_service node loads as its primary language model."}),"\n",(0,s.jsx)(n.p,{children:"Open the file in Terminal :"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\nvim\n~/yahboom_ws/src/largemodel/config/HemiHex.yaml\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"vim\n~/yahboom_ws/src/largemodel/config/HemiHex.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:"Modify/confirm llm_platform :"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\nmodel_service\n:\n#\u6a21\u578b\u670d\u52a1\u5668\u8282\u70b9\u53c2\u6570 Model server node parameters\nros__parameters\n:\nlanguage\n:\n'en'\n#\u5927\u6a21\u578b\u63a5\u53e3\u8bed\u8a00 Large Model Interface Language\nuseolinetts\n:\nFalse\n#\u6587\u5b57\u6a21\u5f0f\u4e0b\u6b64\u9879\u65e0\u6548\uff0c\u53ef\u5ffd\u7565 This item is invalid in text mode and can be ignored\n\u200b\n# \u5927\u6a21\u578b\u914d\u7f6e Large model configuration\nllm_platform\n:\n'ollama'\n# \u5173\u952e: \u786e\u4fdd\u8fd9\u91cc\u662f 'ollama' Key: Make sure it's 'ollama'\nregional_setting\n:\n\"international\"\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"model_service\n:\n#\u6a21\u578b\u670d\u52a1\u5668\u8282\u70b9\u53c2\u6570 Model server node parameters\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros__parameters\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"language\n:\n'en'\n#\u5927\u6a21\u578b\u63a5\u53e3\u8bed\u8a00 Large Model Interface Language\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"useolinetts\n:\nFalse\n#\u6587\u5b57\u6a21\u5f0f\u4e0b\u6b64\u9879\u65e0\u6548\uff0c\u53ef\u5ffd\u7565 This item is invalid in text mode and can be ignored\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# \u5927\u6a21\u578b\u914d\u7f6e Large model configuration\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"llm_platform\n:\n'ollama'\n# \u5173\u952e: \u786e\u4fdd\u8fd9\u91cc\u662f 'ollama' Key: Make sure it's 'ollama'\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'regional_setting\n:\n"international"\n'})}),"\n",(0,s.jsx)(n.h4,{id:"312-configuring-the-model-interface--large_model_interfaceyaml-",children:"3.1.2 Configuring the Model Interface ( large_model_interface.yaml )"}),"\n",(0,s.jsx)(n.p,{children:"This file defines which vision model to use when the ollama platform is selected."}),"\n",(0,s.jsx)(n.p,{children:"1.Open the file in Terminal"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\nvim ~/yahboom_ws/src/largemodel/config/large_model_interface.yaml\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"vim ~/yahboom_ws/src/largemodel/config/large_model_interface.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:"2.Find the ollama related configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'xxxxxxxxxx\n#.....\n## \u79bb\u7ebf\u5927\u6a21\u578b (Offline Large Language Models)\n# Ollama\u914d\u7f6e Ollama Configuration\nollama_host:\n"http://localhost:11434"\n# Ollama\u670d\u52a1\u5668\u5730\u5740 Ollama server address\nollama_model:\n"llava"\n# \u5173\u952e: \u5c06\u8fd9\u91cc\u6539\u4e3a\u4f60\u5df2\u4e0b\u8f7d\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5982 "llava" Key: Change this to the multimodal model you downloaded, such as "llava"\n#.....\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"#.....\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"## \u79bb\u7ebf\u5927\u6a21\u578b (Offline Large Language Models)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Ollama\u914d\u7f6e Ollama Configuration\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'ollama_host:\n"http://localhost:11434"\n# Ollama\u670d\u52a1\u5668\u5730\u5740 Ollama server address\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'ollama_model:\n"llava"\n# \u5173\u952e: \u5c06\u8fd9\u91cc\u6539\u4e3a\u4f60\u5df2\u4e0b\u8f7d\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5982 "llava" Key: Change this to the multimodal model you downloaded, such as "llava"\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"#.....\n"})}),"\n",(0,s.jsx)(n.p,{children:"Note : Please make sure that the model specified in the configuration parameters (such as llava ) can handle multimodal input."}),"\n",(0,s.jsx)(n.h3,{id:"32-starting-and-testing-the-function-text-input-mode",children:"3.2 Starting and Testing the Function (Text Input Mode)"}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsx)(n.p,{children:"Due to performance limitations, the performance of the Jetson Orin Nano 4GB is poor. To experience this function, please refer to the corresponding section in [Online Large Model (Text Interaction)]"})}),"\n",(0,s.jsx)(n.p,{children:"Start the largemodel main program (text mode) : Open a terminal and run the following command:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\nros2 launch largemodel largemodel_control.launch.py text_chat_mode:\n=\ntrue\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch largemodel largemodel_control.launch.py text_chat_mode:\n=\ntrue\n"})}),"\n",(0,s.jsx)(n.p,{children:"Send text command : Open another terminal again and run the following command,"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\nros2 run text_chat text_chat\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 run text_chat text_chat\n"})}),"\n",(0,s.jsx)(n.p,{children:'Then start typing the text: "What do you see".'}),"\n",(0,s.jsx)(n.p,{children:"Observation : In the first terminal where you run the main program, you will see log output indicating that the system received the text command, invoked the seewhat tool, and ultimately printed out the text description of the desktop generated by the LLaVA model."}),"\n",(0,s.jsx)(n.h2,{id:"4-common-problems-and-solutions",children:"4. Common Problems and Solutions"}),"\n",(0,s.jsx)(n.h4,{id:"problem-1-the-log-displays-failed-to-call-ollama-vision-model-or-the-connection-is-refused",children:'Problem 1: The log displays "Failed to call ollama vision model" or the connection is refused.'}),"\n",(0,s.jsx)(n.p,{children:"Solution :"}),"\n",(0,s.jsx)(n.h4,{id:"problem-2-the-seewhat-tool-returns-unable-to-open-camera-or-fails-to-capture",children:'Problem 2: The seewhat tool returns "Unable to open camera" or fails to capture.'}),"\n",(0,s.jsx)(n.p,{children:"Solution :"})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);