"use strict";(globalThis.webpackChunkhemihex_docs=globalThis.webpackChunkhemihex_docs||[]).push([[7519],{28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>t});var l=i(96540);const s={},o=l.createContext(s);function a(e){const n=l.useContext(o);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),l.createElement(o.Provider,{value:n},e.children)}},56032:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>t,default:()=>h,frontMatter:()=>a,metadata:()=>l,toc:()=>d});const l=JSON.parse('{"id":"HH-101/Offline AI Model/multimodal-visual-understanding-11-offlineaimodel-11-11","title":"Multimodal Visual Understanding Application","description":"1. Concept Introduction","source":"@site/docs/hh101/HH-101/11 - Offline AI Model/11-multimodal-visual-understanding-11-offlineaimodel-11-11.md","sourceDirName":"HH-101/11 - Offline AI Model","slug":"/HH-101/Offline AI Model/multimodal-visual-understanding-11-offlineaimodel-11-11","permalink":"/hh-101/HH-101/Offline AI Model/multimodal-visual-understanding-11-offlineaimodel-11-11","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"title":"Multimodal Visual Understanding Application","sidebar_position":11},"sidebar":"hh101Sidebar","previous":{"title":"Multimodal Autonomous Proxy Application","permalink":"/hh-101/HH-101/Offline AI Model/multimodal-autonomous-proxy-application-11-offlineaimodel-11-25"},"next":{"title":"Multimodal Autonomous Proxy Application","permalink":"/hh-101/HH-101/Offline AI Model/ai-large-model-offline-voice-assistant-11-offlineaimodel-11-26"}}');var s=i(74848),o=i(28453);const a={title:"Multimodal Visual Understanding Application",sidebar_position:11},t="Multimodal Visual Understanding Application",r={},d=[{value:"1. Concept Introduction",id:"1-concept-introduction",level:2},{value:"1.1 What is &quot;Visual Understanding&quot;?",id:"11-what-is-visual-understanding",level:3},{value:"1.2 Implementation Principle Overview",id:"12-implementation-principle-overview",level:3},{value:"2. Code Analysis",id:"2-code-analysis",level:2},{value:"Key Code",id:"key-code",level:3},{value:"2.1 Tool Layer Entry (<code>largemodel/utils/tools_manager.py</code>)",id:"21-tool-layer-entry-largemodelutilstools_managerpy",level:4},{value:"2.2 Model Interface Layer (<code>largemodel/utils/large_model_interface.py</code>)",id:"22-model-interface-layer-largemodelutilslarge_model_interfacepy",level:4},{value:"Code Architecture Summary",id:"code-architecture-summary",level:3},{value:"3.1 Configuring the Offline Large Model",id:"31-configuring-the-offline-large-model",level:2},{value:"3.1.1 Configuring the LLM Platform (<code>hemihex.yaml</code>)",id:"311-configuring-the-llm-platform-hemihexyaml",level:3},{value:"3.1.2 Configuring the Model Interface (<code>large_model_interface.yaml</code>)",id:"312-configuring-the-model-interface-large_model_interfaceyaml",level:3},{value:"3.2 Starting and Testing the Function (Text Input Mode)",id:"32-starting-and-testing-the-function-text-input-mode",level:2},{value:"4. Common Problems and Solutions",id:"4-common-problems-and-solutions",level:2},{value:"Problem 1: The log displays &quot;Failed to call ollama vision model&quot; or the connection is refused",id:"problem-1-the-log-displays-failed-to-call-ollama-vision-model-or-the-connection-is-refused",level:3},{value:"Problem 2: The <code>seewhat</code> tool returns &quot;Unable to open camera&quot; or fails to capture",id:"problem-2-the-seewhat-tool-returns-unable-to-open-camera-or-fails-to-capture",level:3}];function c(e){const n={admonition:"admonition",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"multimodal-visual-understanding-application",children:"Multimodal Visual Understanding Application"})}),"\n",(0,s.jsx)(n.h2,{id:"1-concept-introduction",children:"1. Concept Introduction"}),"\n",(0,s.jsx)(n.h3,{id:"11-what-is-visual-understanding",children:'1.1 What is "Visual Understanding"?'}),"\n",(0,s.jsxs)(n.p,{children:["In the ",(0,s.jsx)(n.code,{children:"largemodel"})," project, the ",(0,s.jsx)(n.strong,{children:"multimodal visual understanding"})," feature enables a robot to go beyond simply processing pixels and instead ",(0,s.jsx)(n.strong,{children:"understand objects, scenes, and relationships"})," within an image. This capability allows the system to generate meaningful, natural-language descriptions of what it observes."]}),"\n",(0,s.jsxs)(n.p,{children:["The core tool enabling this feature is ",(0,s.jsx)(n.code,{children:"seewhat"}),". When a user issues a command such as ",(0,s.jsx)(n.strong,{children:'"see what\'s here"'}),", this tool is invoked to capture a live image and analyze it using a multimodal AI model."]}),"\n",(0,s.jsx)(n.h3,{id:"12-implementation-principle-overview",children:"1.2 Implementation Principle Overview"}),"\n",(0,s.jsxs)(n.p,{children:["This feature combines ",(0,s.jsx)(n.strong,{children:"visual information (images)"})," and ",(0,s.jsx)(n.strong,{children:"linguistic information (text)"})," and feeds them into a multimodal large model (for example, LLaVA)."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Image Encoding"}),(0,s.jsx)(n.br,{}),"\n","The model uses a vision encoder to convert the input image into digital feature vectors describing color, shape, and texture."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Text Encoding"}),(0,s.jsx)(n.br,{}),"\n","The user's question (for example, ",(0,s.jsx)(n.strong,{children:'"What\'s on the table?"'}),") is encoded into a text vector."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Cross-modal Fusion"}),(0,s.jsx)(n.br,{}),"\n","Image and text vectors are fused using an attention mechanism. The model learns which regions of the image are relevant to the user's question."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Answer Generation"}),(0,s.jsx)(n.br,{}),"\n","A large language model generates a natural-language description based on the fused visual and textual information."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["In short, the system ",(0,s.jsx)(n.strong,{children:"aligns text with relevant regions of the image and then describes those regions using language"}),"."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"2-code-analysis",children:"2. Code Analysis"}),"\n",(0,s.jsx)(n.h3,{id:"key-code",children:"Key Code"}),"\n",(0,s.jsxs)(n.h4,{id:"21-tool-layer-entry-largemodelutilstools_managerpy",children:["2.1 Tool Layer Entry (",(0,s.jsx)(n.code,{children:"largemodel/utils/tools_manager.py"}),")"]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"seewhat"})," function defines the execution flow of the visual understanding tool."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# From largemodel/utils/tools_manager.py\n\nclass ToolsManager:\n    # ...\n\n    def seewhat(self):\n        """\n        Capture a camera frame and analyze the environment with an AI model.\n\n        :return: A dictionary containing the scene description and image path,\n                 or None if the operation fails.\n        """\n        self.node.get_logger().info("Executing seewhat() tool")\n        image_path = self.capture_frame()\n\n        if image_path:\n            # Use an isolated context for image analysis.\n            analysis_text = self._get_actual_scene_description(image_path)\n\n            # Return structured data for the tool chain.\n            return {\n                "description": analysis_text,\n                "image_path": image_path\n            }\n        else:\n            # Error handling\n            return None\n\n    def _get_actual_scene_description(self, image_path, message_context=None):\n        """\n        Get an AI-generated scene description for the captured image.\n\n        :param image_path: Path to the captured image file.\n        :return: Plain-text description of the scene.\n        """\n        try:\n            # Build the prompt (omitted here for brevity)\n            result = self.node.model_client.infer_with_image(\n                image_path,\n                scene_prompt,\n                message=simple_context\n            )\n            # Process the result (omitted)\n            return description\n        except Exception:\n            # Error handling\n            pass\n'})}),"\n",(0,s.jsxs)(n.h4,{id:"22-model-interface-layer-largemodelutilslarge_model_interfacepy",children:["2.2 Model Interface Layer (",(0,s.jsx)(n.code,{children:"largemodel/utils/large_model_interface.py"}),")"]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"infer_with_image"})," function is the unified entry point for all image-understanding tasks. It dispatches requests based on the configured model platform."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# From largemodel/utils/large_model_interface.py\n\nclass model_interface:\n    # ...\n\n    def infer_with_image(self, image_path, text=None, message=None):\n        """Unified image inference interface."""\n        # Prepare messages (omitted)\n        try:\n            # Select implementation based on configured platform\n            if self.llm_platform == \'ollama\':\n                response_content = self.ollama_infer(\n                    self.messages,\n                    image_path=image_path\n                )\n            elif self.llm_platform == \'tongyi\':\n                # Logic for the Tongyi platform\n                pass\n\n            return {\n                "response": response_content,\n                "messages": self.messages.copy()\n            }\n        except Exception:\n            # Error handling\n            pass\n'})}),"\n",(0,s.jsx)(n.h3,{id:"code-architecture-summary",children:"Code Architecture Summary"}),"\n",(0,s.jsxs)(n.p,{children:["The implementation follows a ",(0,s.jsx)(n.strong,{children:"two-layer architecture"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:["Tool Layer (",(0,s.jsx)(n.code,{children:"tools_manager.py"}),")"]}),(0,s.jsx)(n.br,{}),"\n","Defines ",(0,s.jsx)(n.em,{children:"what"})," the system does: capture images, prepare prompts, and request analysis."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsxs)(n.strong,{children:["Model Interface Layer (",(0,s.jsx)(n.code,{children:"large_model_interface.py"}),")"]}),(0,s.jsx)(n.br,{}),"\n","Defines ",(0,s.jsx)(n.em,{children:"how"})," the system communicates with the AI model and selects the appropriate backend."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["This separation allows the same business logic to work across ",(0,s.jsx)(n.strong,{children:"offline and online AI platforms"})," without code changes."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"31-configuring-the-offline-large-model",children:"3.1 Configuring the Offline Large Model"}),"\n",(0,s.jsxs)(n.h3,{id:"311-configuring-the-llm-platform-hemihexyaml",children:["3.1.1 Configuring the LLM Platform (",(0,s.jsx)(n.code,{children:"hemihex.yaml"}),")"]}),"\n",(0,s.jsx)(n.p,{children:"This configuration file determines which large-model platform is used by the model service."}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Open the configuration file"}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"vim ~/hemihex_ws/src/largemodel/config/hemihex.yaml\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Confirm or modify the platform setting"}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'model_service:\n  ros__parameters:\n    language: "en"               # Large model interface language\n    useolinetts: false           # Not used in text-only mode; can be ignored if not applicable\n    llm_platform: "ollama"       # Key: set to "ollama" for offline mode\n    regional_setting: "international"\n'})}),"\n",(0,s.jsxs)(n.h3,{id:"312-configuring-the-model-interface-large_model_interfaceyaml",children:["3.1.2 Configuring the Model Interface (",(0,s.jsx)(n.code,{children:"large_model_interface.yaml"}),")"]}),"\n",(0,s.jsxs)(n.p,{children:["This file defines which vision model is used when the ",(0,s.jsx)(n.code,{children:"ollama"})," platform is selected."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Open the file"}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"vim ~/hemihex_ws/src/largemodel/config/large_model_interface.yaml\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Set or confirm the Ollama vision model"})," (example):"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# Offline Large Models (Ollama)\nollama_model: "llava"  # Set to the multimodal model you downloaded (e.g., "llava")\n'})}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"Ensure the configured model supports multimodal (image + text) input."})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"32-starting-and-testing-the-function-text-input-mode",children:"3.2 Starting and Testing the Function (Text Input Mode)"}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsx)(n.p,{children:"On lower-memory devices (for example, Jetson Orin Nano 4GB), this feature may run slowly or be unstable. For best results, use a higher-performance device or the online model mode if available."})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Start the ",(0,s.jsx)(n.code,{children:"largemodel"})," main program (text mode)"]}),":"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch largemodel largemodel_control.launch.py text_chat_mode:=true\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Send a text command"})," (in a new terminal):"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 run text_chat text_chat\n"})}),"\n",(0,s.jsx)(n.p,{children:"Then type:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"What do you see\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Expected behavior"}),(0,s.jsx)(n.br,{}),"\n","In the terminal running the main program, you should see logs indicating the system received the command, invoked the ",(0,s.jsx)(n.code,{children:"seewhat"})," tool, and printed a text description generated by the vision model."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"4-common-problems-and-solutions",children:"4. Common Problems and Solutions"}),"\n",(0,s.jsx)(n.h3,{id:"problem-1-the-log-displays-failed-to-call-ollama-vision-model-or-the-connection-is-refused",children:'Problem 1: The log displays "Failed to call ollama vision model" or the connection is refused'}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Possible causes"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Ollama service is not running."}),"\n",(0,s.jsx)(n.li,{children:"The configured model is missing or misspelled."}),"\n",(0,s.jsx)(n.li,{children:"Port binding or local firewall restrictions."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Solution"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Confirm the model exists and Ollama is available:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ollama list\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:"Re-check configuration values:"}),"\n"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"hemihex.yaml"}),": ",(0,s.jsx)(n.code,{children:'llm_platform: "ollama"'})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"large_model_interface.yaml"}),": ",(0,s.jsx)(n.code,{children:'ollama_model: "llava"'})," (or your chosen model)"]}),"\n"]}),"\n",(0,s.jsxs)(n.h3,{id:"problem-2-the-seewhat-tool-returns-unable-to-open-camera-or-fails-to-capture",children:["Problem 2: The ",(0,s.jsx)(n.code,{children:"seewhat"}),' tool returns "Unable to open camera" or fails to capture']}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Possible causes"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Camera not detected."}),"\n",(0,s.jsx)(n.li,{children:"Camera is busy (used by another app)."}),"\n",(0,s.jsx)(n.li,{children:"Permission or device access issue."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Solution"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Verify the camera device exists:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ls /dev/video*\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsxs)(n.li,{children:["Test the camera with a viewer app (for example, ",(0,s.jsx)(n.code,{children:"cheese"})," or ",(0,s.jsx)(n.code,{children:"guvcview"}),") and close any applications using the camera before retrying."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:["This documentation is maintained by ",(0,s.jsx)(n.strong,{children:"HemiHex"})," and describes a modular, platform-agnostic approach to multimodal visual understanding on Jetson-based systems."]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);