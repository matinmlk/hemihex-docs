"use strict";(globalThis.webpackChunkhemihex_docs=globalThis.webpackChunkhemihex_docs||[]).push([[7950],{28453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>s});var t=i(96540);const o={},a=t.createContext(o);function l(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:l(e.components),t.createElement(a.Provider,{value:n},e.children)}},63635:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>s,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"HH-101/Online AI Model/Multi-Module-Visual-Position-Application","title":"Multi module visual position application","description":"1. Concept Introduction","source":"@site/docs/hh101/HH-101/12 - Online AI Model/09-Multi-Module-Visual-Position-Application.md","sourceDirName":"HH-101/12 - Online AI Model","slug":"/HH-101/Online AI Model/Multi-Module-Visual-Position-Application","permalink":"/hh-101/HH-101/Online AI Model/Multi-Module-Visual-Position-Application","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"title":"Multi module visual position application","sidebar_position":9},"sidebar":"hh101Sidebar","previous":{"title":"Multimodal visual understand speech interaction","permalink":"/hh-101/HH-101/Online AI Model/Multimodal-Visual-Understanding-Speech-Interaction"},"next":{"title":"Multimodal table scanning application-10","permalink":"/hh-101/HH-101/Online AI Model/Multimodal-Table-Scanning-Application-v2"}}');var o=i(74848),a=i(28453);const l={title:"Multi module visual position application",sidebar_position:9},s="5.Multi module visual position application",r={},c=[{value:"1. Concept Introduction",id:"1-concept-introduction",level:2},{value:"1.1 What is &quot;Multimodal Visual Localization&quot;?",id:"11-what-is-multimodal-visual-localization",level:3},{value:"1.2 Brief Overview of Implementation Principles",id:"12-brief-overview-of-implementation-principles",level:3},{value:"2. Code Analysis",id:"2-code-analysis",level:2},{value:"Key Code",id:"key-code",level:3},{value:"1. Tools Layer Entry ( largemodel/utils/tools_manager.py )",id:"1-tools-layer-entry--largemodelutilstools_managerpy-",level:4},{value:"2. Model interface layer ( largemodel/utils/large_model_interface.py )",id:"2-model-interface-layer--largemodelutilslarge_model_interfacepy-",level:4},{value:"Code Analysis",id:"code-analysis",level:3},{value:"3. Practical Application",id:"3-practical-application",level:2},{value:"3.1 Configuring Online LLM",id:"31-configuring-online-llm",level:3},{value:"3.2 Launch and Test the Function",id:"32-launch-and-test-the-function",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"5multi-module-visual-position-application",children:"5.Multi module visual position application"})}),"\n",(0,o.jsx)(n.h2,{id:"1-concept-introduction",children:"1. Concept Introduction"}),"\n",(0,o.jsx)(n.h3,{id:"11-what-is-multimodal-visual-localization",children:'1.1 What is "Multimodal Visual Localization"?'}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Multimodal visual localization"}),"is a technology that combines multiple sensor inputs (such as cameras, depth sensors, and IMUs) with algorithmic processing techniques to accurately identify and track the position and posture of a device or user in an environment. This technology does not rely solely on a single type of sensor data, but instead integrates information from different perceptual modalities, thereby improving localization accuracy and robustness."]}),"\n",(0,o.jsx)(n.h3,{id:"12-brief-overview-of-implementation-principles",children:"1.2 Brief Overview of Implementation Principles"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cross-modal Representation Learning"})," : In order for LLMs to be capable of processing visual information, a mechanism must be developed to transform visual signals into a form that the model can understand. This may involve extracting features using convolutional neural networks (CNNs) or other architectures suitable for image processing and mapping them into the same embedding space as text."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Joint Training"})," : By designing an appropriate loss function, text and visual data can be trained simultaneously within the same framework, allowing the model to learn to relate to these two modalities. For example, in a question-answering system, answers can be given based on both the provided text question and the associated image content."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Visually Guided Language Generation/Understanding"})," : Once effective cross-modal representations are established, visual information can be leveraged to enhance the capabilities of the language model. For example, when given a photo, the model can not only describe what is happening in the image, but also answer specific questions about the scene and even execute instructions based on visual cues (such as navigating to a location)."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"2-code-analysis",children:"2. Code Analysis"}),"\n",(0,o.jsx)(n.h3,{id:"key-code",children:"Key Code"}),"\n",(0,o.jsx)(n.h4,{id:"1-tools-layer-entry--largemodelutilstools_managerpy-",children:"1. Tools Layer Entry ( largemodel/utils/tools_manager.py )"}),"\n",(0,o.jsxs)(n.p,{children:["The",(0,o.jsx)(n.code,{children:"visual_positioning"}),"function in this file defines the execution flow of the tool, specifically how it constructs a prompt containing the target object name and formatting requirements."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# From largemodel/utils/tools_manager.py\nclass ToolsManager:\n# ...\ndef visual_positioning(self, args):\n"""\nLocate object coordinates in image and save results to MD file.\n\n:param args: Arguments containing image path and object name.\n:return: Dictionary with file path and coordinate data.\n"""\nself.node.get_logger().info(f"Executing visual_positioning() tool with args: {args}")\ntry:\nimage_path = args.get("image_path")\nobject_name = args.get("object_name")\n# ... (Path fallback mechanism and parameter checking)\n\n# Construct a prompt asking the large model to identify the coordinates of the specified object.\nif self.node.language == \'zh\':\nprompt = f"Carefully analyze this image and locate each [object_name]. Return bounding-box coordinates in the required format."  # translated from Chinese\nelse:\nprompt = f"Please carefully analyze this image and find the position of all [object_name]..."\n\n# ... (Build an independent message context)\n\nresult = self.node.model_client.infer_with_image(image_path, prompt, message=message_to_use)\n\n# ... (Process and parse the returned coordinate text)\n\nreturn {\n"file_path": md_file_path,\n"coordinates_content": coordinates_content,\n"explanation_content": explanation_content\n}\n# ... (Error Handling)\n'})}),"\n",(0,o.jsx)(n.h4,{id:"2-model-interface-layer--largemodelutilslarge_model_interfacepy-",children:"2. Model interface layer ( largemodel/utils/large_model_interface.py )"}),"\n",(0,o.jsxs)(n.p,{children:["The",(0,o.jsx)(n.code,{children:"infer_with_image"}),"function in this file is the unified entry point for all image-related tasks."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# From largemodel/utils/large_model_interface.py\n\nclass model_interface:\n# ...\ndef infer_with_image(self, image_path, text=None, message=None):\n\"\"\"Unified image inference interface. \"\"\"\n# ... (Prepare message)\ntry:\n# Determine which specific implementation to call based on the value of self.llm_platform\nif self.llm_platform == 'ollama':\nresponse_content = self.ollama_infer(self.messages, image_path=image_path)\nelif self.llm_platform == 'tongyi':\n# ... Logic for calling the Tongyi model\npass\n# ... (Logic for other platforms)\n# ...\nreturn {'response': response_content, 'messages': self.messages.copy()}\n"})}),"\n",(0,o.jsx)(n.h3,{id:"code-analysis",children:"Code Analysis"}),"\n",(0,o.jsxs)(n.p,{children:["The core of the visual positioning function lies in",(0,o.jsx)(n.strong,{children:"guiding large models to output structured data through precise instructions"}),". It also follows the layered design of the tool layer and the model interface layer."]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.strong,{children:"Tools Layer (tools_manager.py):"})}),"\n"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["The ",(0,o.jsx)(n.code,{children:"visual_positioning"})," function is the core of this function. It accepts two key parameters: ",(0,o.jsx)(n.code,{children:"image_path"})," (the image path) and ",(0,o.jsx)(n.code,{children:"object_name"})," (the name of the object to be positioned)."]}),"\n",(0,o.jsxs)(n.li,{children:["The core operation of this function is ",(0,o.jsx)(n.strong,{children:"building a highly customized prompt"})," . It doesn't simply ask the model to describe an image. Instead, it embeds ",(0,o.jsx)(n.code,{children:"object_name"}),' into a carefully designed template, explicitly instructing the model to "locate each [object_name] in the image," and implicitly or explicitly requires the results to be returned in a specific format (such as an array of coordinates).']}),"\n",(0,o.jsxs)(n.li,{children:["After building the prompt, it calls the ",(0,o.jsx)(n.code,{children:"infer_with_image"})," method of the model interface layer, passing the image and this customized instruction. * After receiving the returned text from the model interface layer, it needs to perform ",(0,o.jsx)(n.strong,{children:"post-processing"})," : using methods such as regular expressions to parse the model's natural language response to extract precise coordinate data."]}),"\n",(0,o.jsx)(n.li,{children:"Finally, it returns the parsed structured coordinate data to the upper-layer application."}),"\n"]}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Model Interface Layer (large_model_interface.py)"}),":"]}),"\n"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["The ",(0,o.jsx)(n.code,{children:"infer_with_image"}),' function still serves as the "dispatching center." It receives the image and prompt from ',(0,o.jsx)(n.code,{children:"visual_positioning"})," and dispatches the task to the correct backend model implementation based on the current configuration ( ",(0,o.jsx)(n.code,{children:"self.llm_platform"})," )."]}),"\n",(0,o.jsx)(n.li,{children:"For visual positioning tasks, the model interface layer's responsibilities are essentially the same as for visual understanding tasks: correctly packaging the image data and text instructions, sending them to the selected model platform, and then returning the returned text results intact to the tool layer. All platform-specific implementation details are encapsulated in this layer."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"In summary, the general workflow for visual localization is: ToolsManager receives the target object name and constructs a precise prompt requesting coordinates. ToolsManager calls the model interface. ModelInterface packages the image and prompt together and sends them to the corresponding model platform according to the configuration. The model returns a text file containing the coordinates. ModelInterface returns this text file to ToolsManager. ToolsManager parses the text file, extracts the structured coordinate data, and returns it. This process demonstrates how Prompt Engineering can be used to enable a general large-scale visual model to accomplish more specific and structured tasks."}),"\n",(0,o.jsx)(n.h2,{id:"3-practical-application",children:"3. Practical Application"}),"\n",(0,o.jsx)(n.h3,{id:"31-configuring-online-llm",children:"3.1 Configuring Online LLM"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.strong,{children:"First, obtain the API key from any platform described in the previous tutorial."})}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Next, update the key in the configuration file. Open the model interface configuration file"}),",",(0,o.jsx)(n.code,{children:"large_model_interface.yaml"}),": xxxxxxxxxxvim~/hemihex_ws/src/largemodel/config/large_model_interface.yaml"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Enter your API Key"}),":Find",' the corresponding section and paste the API Key you just copied. This example uses the Tongyi Qianwen configuration. xxxxxxxxxx# large_model_interface.yaml## Thousand Questions on Tongyiqianwen_api_key:"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"# Paste your Keyqianwen_model:"qwen-vl-max-latest"# You can choose the model as needed, such as qwen-turbo, qwen-plus']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Open the main configuration file HemiHex.yaml"}),": xxxxxxxxxxvim~/hemihex_ws/src/largemodel/config/HemiHex.yaml"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Select the online platform you want to use"}),":Change"," the",(0,o.jsx)(n.code,{children:"llm_platform"}),"parameter to the platform name you want to use. xxxxxxxxxx# HemiHex.yamlmodel_service:ros__parameters:# ...llm_platform:'tongyi'#Optional platforms: 'ollama', 'tongyi', 'spark', 'qianfan', 'openrouter'"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"32-launch-and-test-the-function",children:"3.2 Launch and Test the Function"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Prepare the Image File"}),": Place the image file to be tested in the following directory:",(0,o.jsx)(n.code,{children:"/home/jetson/hemihex_ws/src/largemodel/resources_file/visual_positioning"})," Then name the image",(0,o.jsx)(n.code,{children:"test_image.jpg"})]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Start the largemodel main program"}),": Open a terminal and run the following command: xxxxxxxxxxros2 launch largemodel largemodel_control.launch.py"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Test"}),":"]}),"\n"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Wake up"}),' : Say "Hello, Xiaoya" into the microphone.']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Dialogue"}),' : After the speaker responds, you can say, "Analyze the position of the dinosaur in the image."']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Observe the log"})," : In the terminal running the ",(0,o.jsx)(n.code,{children:"launch"})," file, you should see the following:"]}),"\n"]}),"\n",(0,o.jsxs)(n.ol,{start:"4",children:["\n",(0,o.jsx)(n.li,{children:"The ASR node recognizes your question and prints it out."}),"\n",(0,o.jsxs)(n.li,{children:["The",(0,o.jsx)(n.code,{children:"model_service"}),"node receives the text, calls the LLM, and prints the LLM's response."]}),"\n"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Listen for the response"})," : After a while, you should hear the response from the speaker and find an md file in the ",(0,o.jsx)(n.code,{children:"/home/jetson/hemihex_ws/src/largemodel/resources_file/visual_positioning"})," path that records the coordinate position and positioned object information."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);