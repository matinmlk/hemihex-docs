"use strict";(globalThis.webpackChunkhemihex_docs=globalThis.webpackChunkhemihex_docs||[]).push([[1691],{28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>l});var s=t(96540);const a={},i=s.createContext(a);function o(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(i.Provider,{value:n},e.children)}},52711:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"HH-101/Online AI Model/Multimodal-Autonomous-Proxy-Application-v2","title":"Multimodal autonomous proxy application-11","description":"1. Concept Introduction","source":"@site/docs/hh101/HH-101/12 - Online AI Model/11-Multimodal-Autonomous-Proxy-Application-v2.md","sourceDirName":"HH-101/12 - Online AI Model","slug":"/HH-101/Online AI Model/Multimodal-Autonomous-Proxy-Application-v2","permalink":"/hh-101/HH-101/Online AI Model/Multimodal-Autonomous-Proxy-Application-v2","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"title":"Multimodal autonomous proxy application-11","sidebar_position":11},"sidebar":"hh101Sidebar","previous":{"title":"Multimodal table scanning application-10","permalink":"/hh-101/HH-101/Online AI Model/Multimodal-Table-Scanning-Application-v2"},"next":{"title":"Online speech to text (ASR)","permalink":"/hh-101/HH-101/Online AI Model/Online-Speech-to-Text-ASR"}}');var a=t(74848),i=t(28453);const o={title:"Multimodal autonomous proxy application-11",sidebar_position:11},l="7.Multimodal autonomous proxy application",r={},c=[{value:"1. Concept Introduction",id:"1-concept-introduction",level:2},{value:"1.1 What is an &quot;Autonomous Agent&quot;?",id:"11-what-is-an-autonomous-agent",level:3},{value:"1.2 Implementation Principles",id:"12-implementation-principles",level:3},{value:"2. Project Architecture",id:"2-project-architecture",level:2},{value:"Key Code",id:"key-code",level:3},{value:"1. Agent Core Workflow ( largemodel/utils/ai_agent.py )",id:"1-agent-core-workflow--largemodelutilsai_agentpy-",level:4},{value:"2. Task Planning and LLM Interaction ( largemodel/utils/ai_agent.py )",id:"2-task-planning-and-llm-interaction--largemodelutilsai_agentpy-",level:4},{value:"3. Parameter processing and data flow implementation( largemodel/utils/ai_agent.py )",id:"3-parameter-processing-and-data-flow-implementation-largemodelutilsai_agentpy-",level:4},{value:"Code Analysis",id:"code-analysis",level:3},{value:"3. Practical Practice",id:"3-practical-practice",level:2},{value:"3.1 Configuring Online LLM",id:"31-configuring-online-llm",level:3},{value:"3.2 Launching and Testing the Functionality",id:"32-launching-and-testing-the-functionality",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"7multimodal-autonomous-proxy-application",children:"7.Multimodal autonomous proxy application"})}),"\n",(0,a.jsx)(n.h2,{id:"1-concept-introduction",children:"1. Concept Introduction"}),"\n",(0,a.jsx)(n.h3,{id:"11-what-is-an-autonomous-agent",children:'1.1 What is an "Autonomous Agent"?'}),"\n",(0,a.jsxs)(n.p,{children:["In the ",(0,a.jsx)(n.code,{children:"largemodel"})," project, ",(0,a.jsx)(n.strong,{children:"multimodal autonomous agents"})," represent the most advanced form of intelligence. Rather than simply responding to a user's command, they are capable of ",(0,a.jsx)(n.strong,{children:"autonomously thinking, planning, and sequentially invoking multiple tools to achieve a complex goal"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["The core of this functionality is the ",(0,a.jsx)(n.code,{children:"agent_call"})," tool or its underlying **ToolChainManager`. When a user issues a complex request that cannot be accomplished with a single tool call, the autonomous agent is activated."]}),"\n",(0,a.jsx)(n.h3,{id:"12-implementation-principles",children:"1.2 Implementation Principles"}),"\n",(0,a.jsxs)(n.p,{children:["The autonomous agent implementation in ",(0,a.jsx)(n.code,{children:"largemodel"})," follows the industry-leading ",(0,a.jsx)(n.strong,{children:"ReAct (Reason + Act)"}),' paradigm. Its core concept is to mimic the human problem-solving process, cycling between "thinking" and "acting."']}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reason"}),' : When the agent receives a complex goal, it first "reasons" by invoking a powerful language model (LLM). It asks itself, "What should I do first to achieve this goal? Which tool should I use?" The LLM\'s output isn\'t a final answer, but rather an action plan.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Act"})," : Based on the LLM's reasoning, the agent takes the appropriate action\u2014calling the ToolsManager to run a specified tool (such as visual_positioning)."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Observe"}),' : The agent obtains the result of the previous action (the "observation"), for example, ["result": "The cup was found at [120, 300, 180, 360]"].']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Rethink"}),' : The agent submits this observation, along with the original goal, to the LLM for a second round of "reflection." It asks itself, "I\'ve found the location of the cup. What should I do next to learn its color?" The LLM might generate a new action plan, such as ',(0,a.jsx)(n.code,{children:'{"thought": "I need to analyze the image of the area where the cup is located to determine its color", "action": "seewhat", "args": {"crop_area": [120, 300, 180, 360]}}'})," ."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["This ",(0,a.jsx)(n.strong,{children:"think -> act -> observe"})," cycle continues until the initial goal is achieved, at which point the agent generates and outputs the final answer."]}),"\n",(0,a.jsx)(n.h2,{id:"2-project-architecture",children:"2. Project Architecture"}),"\n",(0,a.jsx)(n.h3,{id:"key-code",children:"Key Code"}),"\n",(0,a.jsx)(n.h4,{id:"1-agent-core-workflow--largemodelutilsai_agentpy-",children:"1. Agent Core Workflow ( largemodel/utils/ai_agent.py )"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"_execute_agent_workflow"}),' function is the agent\'s main execution loop, defining the core "plan -> execute" process.']}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# From largemodel/utils/ai_agent.py\n\nclass AIAgent:\n    # ...\n\n    def _execute_agent_workflow(self, task_description: str) -> Dict[str, Any]:\n        """\n        Executes the agent workflow: Plan -> Execute.\n        """\n        try:\n            # Step 1: Task Planning\n            self.node.get_logger().info("AI Agent starting task planning phase")\n            plan_result = self._plan_task(task_description)\n\n            # ... (Return early if planning fails)\n\n            self.task_steps = plan_result["steps"]\n\n            # Step 2: Follow all steps in order\n            execution_results = []\n            tool_outputs = []\n\n            for i, step in enumerate(self.task_steps):\n                # 2.1. Processing data references in parameters before execution\n                processed_parameters = self._process_step_parameters(step.get("parameters", {}), tool_outputs)\n                step["parameters"] = processed_parameters\n\n                # 2.2. Execute a single step\n                step_result = self._execute_step(step, tool_outputs)\n                execution_results.append(step_result)\n\n                # 2.3. If the step succeeds, save its output for reference in subsequent steps\n                if step_result.get("success") and step_result.get("tool_output"):\n                    tool_outputs.append(step_result["tool_output"])\n                else:\n                    # If any step fails, abort the entire task\n                    return { "success": False, "message": f"Task terminated because step \'{step[\'description\']}\' failed." }\n\n            # ... Summarize and return the final result\n            summary = self._summarize_execution(task_description, execution_results)\n            return { "success": True, "message": summary, "results": execution_results }\n\n        # ... (Exception handling)\n'})}),"\n",(0,a.jsx)(n.h4,{id:"2-task-planning-and-llm-interaction--largemodelutilsai_agentpy-",children:"2. Task Planning and LLM Interaction ( largemodel/utils/ai_agent.py )"}),"\n",(0,a.jsxs)(n.p,{children:["The core of the ",(0,a.jsx)(n.code,{children:"_plan_task"})," function is to build a sophisticated prompt, leveraging the large model's inherent reasoning capabilities to generate a structured execution plan."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# From largemodel/utils/ai_agent.py\n\nclass AIAgent:\n    # ...\n    def _plan_task(self, task_description: str) -> Dict[str, Any]:\n        """\n        Uses the large model for task planning and decomposition.\n        """\n        # Dynamically generate a list of available tools and their descriptions\n        tool_descriptions = []\n        for name, adapter in self.tools_manager.tool_chain_manager.tools.items():\n            # ... (Get the tool description from adapter.input_schema)\n            tool_descriptions.append(f"- {name}({params}): {description}")\n        available_tools_str = "\\\\n".join(tool_descriptions)\n\n        # Build a highly structured plan\n        planning_prompt = f"""\nAs a professional task planning agent, please break down user tasks into a series of specific, executable JSON steps.\n\n**# Available Tools:**\n{available_tools_str}\n\n**# Core Rules:**\n1. **Data Passing**: When a subsequent step requires the output of a previous step, it must be referenced using the `{{{{steps.N.outputs.KEY}}}}` format.\n- `N` is the step ID (starting at 1).\n- `KEY` is the specific field name in the output data of the previous step.\n2. **JSON Format**: Must strictly return a JSON object.\n\n**# User Tasks:**\n{task_description}\n"""\n\n        # Calling large models for planning\n        messages_to_use = [{"role": "user", "content": planning_prompt}]\n        # Note that the general text reasoning interface is called here\n        result = self.node.model_client.infer_with_text("", message=messages_to_use)\n\n        # ... (parses the JSON response and returns a list of steps)\n'})}),"\n",(0,a.jsx)(n.h4,{id:"3-parameter-processing-and-data-flow-implementation-largemodelutilsai_agentpy-",children:"3. Parameter processing and data flow implementation( largemodel/utils/ai_agent.py )"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"_process_step_parameters"}),"The function is responsible for parsing placeholders and realizing data flow between steps."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# From largemodel/utils/ai_agent.py\n\nclass AIAgent:\n    # ...\n    def _process_step_parameters(self, parameters: Dict[str, Any], previous_outputs: List[Any]) -> Dict[str, Any]:\n        """\n        Parses parameter dictionary, finds and replaces all {{...}} references.\n        """\n        processed_params = parameters.copy()\n        # Regular expression used to match placeholders in the format {{steps.N.outputs.KEY}}\n        pattern = re.compile(r"\\\\{\\\\{steps\\\\.(\\\\d+)\\\\.outputs\\\\.(.+?)\\\\}\\\\}")\n\n        for key, value in processed_params.items():\n            if isinstance(value, str) and pattern.search(value):\n                # Use re.sub and a replacement function to process all found placeholders\n                # The replacement function will look up and return the value from the previous_outputs list\n                processed_params[key] = pattern.sub(replacer_function, value)\n\n        return processed_params\n'})}),"\n",(0,a.jsx)(n.h3,{id:"code-analysis",children:"Code Analysis"}),"\n",(0,a.jsx)(n.p,{children:'The AI Agent is the "brain" of the system, translating high-level, sometimes ambiguous, tasks posed by the user into a precise, ordered series of tool calls. Its implementation is independent of any specific model platform and built on a general, extensible architecture.'}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Dynamic Task Planning"}),": The Agent's core capability lies in the ",(0,a.jsx)(n.code,{children:"_plan_task"})," function. Rather than relying on hard-coded logic, it dynamically generates task plans by interacting with a larger model."]}),"\n"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Self-Awareness and Prompt Construction"})," : At the beginning of planning, the Agent first examines all available tools and their descriptions. It then packages this tool information, the user's task, and strict rules (such as data transfer format) into a highly structured ",(0,a.jsx)(n.code,{children:"planning_prompt"})," ."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Model as Planner"})," : This prompt is fed into a general text-based model. The model reasoned based on the provided context and returned a multi-step action plan in JSON format. This design is highly scalable: as tools are added or modified in the system, the Agent's planning capabilities are automatically updated without requiring code modifications."]}),"\n"]}),"\n",(0,a.jsxs)(n.ol,{start:"2",children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Toolchain and Data Flow"}),': Real-world tasks often require the collaboration of multiple tools. For example, "take a picture and describe" requires the output (image path) of the "take a picture" tool to be used as the input of the "describe" tool. The AI Agent elegantly implements this through the ',(0,a.jsx)(n.code,{children:"_process_step_parameters"})," function."]}),"\n"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Reference Placeholders"})," : During the planning phase, large models embed special placeholders, such as ",(0,a.jsx)(n.code,{children:"{{steps.1.outputs.data}}"})," , in parameter values where data needs to be passed."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Real-Time Parameter Replacement"})," : In the ",(0,a.jsx)(n.code,{children:"_execute_agent_workflow"})," main loop, ",(0,a.jsx)(n.code,{children:"_process_step_parameters"})," is called before each step. It uses regular expressions to scan all parameters of the current step. Upon discovering a placeholder, it finds the corresponding data from the output list of the previous step and replaces it in real time. This mechanism is key to automating complex tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.ol,{start:"3",children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Supervised Execution and Fault Tolerance"}),": ",(0,a.jsx)(n.code,{children:"_execute_agent_workflow"})," constitutes the Agent's main execution loop. It strictly follows the planned sequence of steps, executing each action sequentially and ensuring data is correctly passed between them."]}),"\n"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Atomic Steps"}),' : Each step is treated as an independent "atomic operation." If any step fails, the entire task chain immediately aborts and reports an error. This ensures system stability and predictability, preventing continued execution in an erroneous state.']}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"In summary, the general implementation of the AI Agent demonstrates an advanced software architecture: rather than solving a problem directly, it builds a framework that enables an external, general-purpose reasoning engine (the large model) to solve the problem. Through two core mechanisms, dynamic programming and data flow management, the Agent orchestrates a series of independent tools into complex workflows capable of completing advanced tasks."}),"\n",(0,a.jsx)(n.h2,{id:"3-practical-practice",children:"3. Practical Practice"}),"\n",(0,a.jsx)(n.h3,{id:"31-configuring-online-llm",children:"3.1 Configuring Online LLM"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"First, obtain an API key from any of the platforms described in the previous tutorials"})}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Next, you need to update the key in the configuration file. Open the model interface configuration file large_model_interface.yaml"}),": xxxxxxxxxxvim ~/hemihex_ws/src/largemodel/config/large_model_interface.yaml"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Enter your API Key"}),':\nFind the corresponding section and paste the API Key you just copied. This example uses the Tongyi Qianwen configuration. xxxxxxxxxx# large_model_interface.yaml## Thousand Questions on Tongyiqianwen_api_key: "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" # Paste your Keyqianwen_model: "qwen-vl-max-latest" # You can choose the model as needed, such as qwen-turbo, qwen-plus']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Open the main configuration file HemiHex.yaml"}),": xxxxxxxxxxvim ~/hemihex_ws/src/largemodel/config/HemiHex.yaml"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Select the online platform you want to use"}),":\nChange the ",(0,a.jsx)(n.code,{children:"llm_platform"})," parameter to the platform name you want to use. xxxxxxxxxx# HemiHex.yamlmodel_service:  ros__parameters:    # ...    llm_platform: 'tongyi'  #Optional platforms: 'ollama', 'tongyi', 'spark', 'qianfan', 'openrouter'"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"32-launching-and-testing-the-functionality",children:"3.2 Launching and Testing the Functionality"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Start the largemodel main program"}),": Open a terminal and run the following command: xxxxxxxxxxros2 launch largemodel largemodel_control.launch.py"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Test"}),":"]}),"\n"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Wake up"}),': Say "Hi,HemiHex" into the microphone.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Dialogue"}),": After the speaker responds, you can say: ",(0,a.jsx)(n.code,{children:"Generate an image similar to the current environment."})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Observe the log"}),": In the terminal running the ",(0,a.jsx)(n.code,{children:"launch"})," file, you should see the following: The system receives the text command, invokes the ",(0,a.jsx)(n.code,{children:"aiagent"})," tool, and then provides a prompt to the LLM. The LLM analyzes the detailed tool invocation steps. For example, in this question, the ",(0,a.jsx)(n.code,{children:"seewhat"})," tool is invoked to obtain the image, which is then parsed by the LLM. The parsed text is then passed to the LLM as the content of the new image, which is then generated."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Listen to the answer"}),": After a while, you should hear the completed answer from the speaker, and the camera feed and the newly generated image will pop up. You can later find the newly generated image in the ",(0,a.jsx)(n.code,{children:"/home/jetson/hemihex_ws/src/largemodel/resources_file/generated_images"})," directory."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);