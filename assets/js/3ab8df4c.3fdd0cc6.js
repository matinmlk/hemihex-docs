"use strict";(globalThis.webpackChunkhemihex_docs=globalThis.webpackChunkhemihex_docs||[]).push([[5388],{28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var i=t(96540);const o={},l=i.createContext(o);function s(e){const n=i.useContext(l);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(l.Provider,{value:n},e.children)}},33310:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"HH-101/Online AI Model/AI-Large-Model-Voice-Interaction","title":"AI large model voice interaction","description":"1. Concept Introduction","source":"@site/docs/hh101/HH-101/12 - Online AI Model/07-AI-Large-Model-Voice-Interaction.md","sourceDirName":"HH-101/12 - Online AI Model","slug":"/HH-101/Online AI Model/AI-Large-Model-Voice-Interaction","permalink":"/hh-101/HH-101/Online AI Model/AI-Large-Model-Voice-Interaction","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"AI large model voice interaction","sidebar_position":7},"sidebar":"hh101Sidebar","previous":{"title":"Voice Interaction Hardware Connection","permalink":"/hh-101/HH-101/Online AI Model/Voice-Interaction-Hardware-Connection"},"next":{"title":"Multimodal visual understand speech interaction","permalink":"/hh-101/HH-101/Online AI Model/Multimodal-Visual-Understanding-Speech-Interaction"}}');var o=t(74848),l=t(28453);const s={title:"AI large model voice interaction",sidebar_position:7},r="3.AI large model voice interaction",a={},c=[{value:"1. Concept Introduction",id:"1-concept-introduction",level:2},{value:"1.1 What is &quot;AI Large Model Voice Interaction&quot;?",id:"11-what-is-ai-large-model-voice-interaction",level:3},{value:"1.2 Implementation Principles",id:"12-implementation-principles",level:3},{value:"2. Project Architecture",id:"2-project-architecture",level:2},{value:"2.1 Key Code Analysis",id:"21-key-code-analysis",level:3},{value:"3. Practice",id:"3-practice",level:2},{value:"3.1 Configuring Online LLM",id:"31-configuring-online-llm",level:3},{value:"3.2 Launching and Testing the Functionality",id:"32-launching-and-testing-the-functionality",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,l.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"3ai-large-model-voice-interaction",children:"3.AI large model voice interaction"})}),"\n",(0,o.jsx)(n.h2,{id:"1-concept-introduction",children:"1. Concept Introduction"}),"\n",(0,o.jsx)(n.h3,{id:"11-what-is-ai-large-model-voice-interaction",children:'1.1 What is "AI Large Model Voice Interaction"?'}),"\n",(0,o.jsxs)(n.p,{children:["In the",(0,o.jsx)(n.code,{children:"largemodel"}),"project,",(0,o.jsx)(n.strong,{children:"AI Large Model Voice Interaction"}),"combines the",(0,o.jsx)(n.strong,{children:"offline ASR"}),"and",(0,o.jsx)(n.strong,{children:"offline TTS"}),"described above with the**large language model (LLM)**core to form a complete conversational system that listens, speaks, and thinks."]}),"\n",(0,o.jsxs)(n.p,{children:["This is no longer an isolated function, but the prototype of a true",(0,o.jsx)(n.strong,{children:"voice assistant"}),". Users can engage in natural language conversations with the robot through voice, and the robot can understand questions, think about answers, and respond with voice. This entire process is completed locally, without the need for a network."]}),"\n",(0,o.jsxs)(n.p,{children:["The core of this function is the",(0,o.jsx)(n.code,{children:"model_service"}),(0,o.jsx)(n.strong,{children:"ROS2 node"}),". It acts as the brain and neural center, subscribing to ASR recognition results, invoking the LLM for thinking, and then publishing the LLM's text responses to the TTS node for speech synthesis."]}),"\n",(0,o.jsx)(n.h3,{id:"12-implementation-principles",children:"1.2 Implementation Principles"}),"\n",(0,o.jsx)(n.p,{children:"This feature is implemented using a classic data flow pipeline:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Audio -> Text (ASR) : The asr node continuously listens to ambient sound. Once it detects a user speaking a sentence, it converts it into text and publishes it to the /asr_text topic."}),"\n",(0,o.jsx)(n.li,{children:"Text -> Thought -> Text (LLM) : The model_service node subscribes to the /asr_text topic. Upon receiving the text from the ASR, it passes it as a prompt to a locally deployed large language model (such as Qwen running through Ollama ). The LLM generates a text response based on the context."}),"\n",(0,o.jsx)(n.li,{children:"Text -> Audio (TTS) : After receiving the LLM's response, the model_service node publishes it to the /tts_text topic."}),"\n",(0,o.jsx)(n.li,{children:"Audio playback : The tts_only node subscribes to the /tts_text topic. Upon receiving text, it immediately invokes the offline TTS model to synthesize it into audio and plays it through the speaker."}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["This process forms a complete closed loop:",(0,o.jsx)(n.strong,{children:"speech input -> text processing -> text output -> speech output"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"2-project-architecture",children:"2. Project Architecture"}),"\n",(0,o.jsx)(n.h3,{id:"21-key-code-analysis",children:"2.1 Key Code Analysis"}),"\n",(0,o.jsxs)(n.p,{children:["The core of the entire process lies in how the",(0,o.jsx)(n.code,{children:"model_service"}),"node connects its input and output."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"1. Subscribing to ASR Results"}),"(",(0,o.jsx)(n.strong,{children:"located in"}),(0,o.jsx)(n.code,{children:"largemodel/model_service.py"}),")The",(0,o.jsx)(n.code,{children:"model_service"}),"node has a subscriber to receive the ASR-recognized text."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# largemodel/model_service.py (Core logic diagram)\nclass ModelService(Node):\ndef __init__(self):\nsuper().__init__('model_service')\n# ...\n# Subscribe to the ASR text output topic\nself.asr_subscription = self.create_subscription(\nString,\n'asr_text',\nself.asr_callback,\n10)\n\n# Create a TTS text input topic publisher\nself.tts_publisher = self.create_publisher(String, 'tts_text', 10)\n\n# Initialize the large model interface\nself.large_model_interface = LargeModelInterface(self)\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Explanation"}),": The node's",(0,o.jsx)(n.code,{children:"__init__"}),'method clearly defines its role: a middleman that listens to ASR results and commands the TTS to speak, with an internal "brain" (',(0,o.jsx)(n.code,{children:"LargeModelInterface"}),")."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"2. Processing ASR Text and Calling the LLM"}),"(",(0,o.jsx)(n.strong,{children:"located in"}),(0,o.jsx)(n.code,{children:"largemodel/model_service.py"}),")When the ASR generates new recognition results,",(0,o.jsx)(n.code,{children:"asr_callback"}),"is triggered."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# largemodel/model_service.py (Core logic diagram)\ndef asr_callback(self, msg):\nuser_text = msg.data\nself.get_logger().info(f'Received from ASR: \"{user_text}\"')\n\n# Calling the large model interface for consideration\n# llm_platform determines whether to call Ollama or the online API\nllm_platform = self.get_parameter('llm_platform').value\nresponse_text = self.large_model_interface.call_llm(user_text, llm_platform)\n\nif response_text:\nself.get_logger().info(f'LLM reply: \"{response_text}\"')\n# Send LLM's reply to TTS\nself.speak(response_text)\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Explanation"}),": This is the core logic of the system. After receiving the text, the callback function immediately sends it to the LLM via",(0,o.jsx)(n.code,{children:"large_model_interface"}),". The",(0,o.jsx)(n.code,{children:"call_llm"}),"method determines whether to connect to the local Ollama or online API based on the",(0,o.jsx)(n.code,{children:"llm_platform"}),"configuration."]}),"\n",(0,o.jsxs)(n.p,{children:["**3. Sending the LLM response to the TTS (located in largemodel/model_service.py)**The",(0,o.jsx)(n.code,{children:"speak"}),"method is a simple wrapper that publishes the text to the topic listened to by the TTS node."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# largemodel/model_service.py (Core logic diagram)\ndef speak(self, text):\nmsg = String()\nmsg.data = text\nself.tts_publisher.publish(msg)\n"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Explanation"}),': This function completes the final step in the data flow, passing the text generated by the "brain" to the "mouth," thus completing the entire voice interaction loop.']}),"\n",(0,o.jsx)(n.h2,{id:"3-practice",children:"3. Practice"}),"\n",(0,o.jsx)(n.h3,{id:"31-configuring-online-llm",children:"3.1 Configuring Online LLM"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"First, obtain an API key from any platform discussed in the previous tutorials"}),"\n",(0,o.jsx)(n.li,{children:"Next, update the key in the configuration file. Open the model interface configuration file large_model_interface.yaml : xxxxxxxxxx vim ~/hemihex_ws/src/largemodel/config/large_model_interface.yaml"}),"\n",(0,o.jsx)(n.li,{children:'Enter your API Key : Find the corresponding section and paste the API Key you just copied. This example uses the Tongyi Qianwen configuration. xxxxxxxxxx # large_model_interface.yaml  ## Thousand Questions on Tongyi qianwen_api_key : "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx" # Paste your Key qianwen_model : "qwen-vl-max-latest" # You can choose the model as needed, such asqwen-turbo, qwen-plus'}),"\n",(0,o.jsx)(n.li,{children:"Open the main configuration file HemiHex.yaml : xxxxxxxxxx vim ~/hemihex_ws/src/largemodel/config/HemiHex.yaml"}),"\n",(0,o.jsx)(n.li,{children:"Select the online platform you want to use : Change the llm_platform parameter to the platform name you want to use. xxxxxxxxxx # HemiHex.yaml  model_service : ros__parameters : # ... llm_platform : 'tongyi' #Optional platforms: 'ollama', 'tongyi', 'spark', 'qianfan', 'openrouter'"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"32-launching-and-testing-the-functionality",children:"3.2 Launching and Testing the Functionality"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Start the largemodel Main Program :"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Run the following command to enable voice interaction:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"ros2 launch largemodel largemodel_control.launch.py\n"})}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:'Test : Wake up : Say "Hi, HemiHex" into the microphone. Talk : After the speaker responds, you can speak your questions. Observe the Log : In the terminal running the launch file, you should see the following: The ASR node recognizes your question and prints it. The model_service node receives the text, calls the LLM, and prints the LLM\'s response. Listen for the Response : After a while, you should hear the response from the speaker.'}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);