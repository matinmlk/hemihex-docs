"use strict";(globalThis.webpackChunkhemihex_docs=globalThis.webpackChunkhemihex_docs||[]).push([[5004],{5583:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"HH-101/Online AI Model/Online-Speech-to-Text-ASR","title":"Online speech to text (ASR)","description":"1. Concept Introduction","source":"@site/docs/hh101/HH-101/12 - Online AI Model/12-Online-Speech-to-Text-ASR.md","sourceDirName":"HH-101/12 - Online AI Model","slug":"/HH-101/Online AI Model/Online-Speech-to-Text-ASR","permalink":"/hh-101/HH-101/Online AI Model/Online-Speech-to-Text-ASR","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"title":"Online speech to text (ASR)","sidebar_position":12},"sidebar":"hh101Sidebar","previous":{"title":"Multimodal autonomous proxy application-11","permalink":"/hh-101/HH-101/Online AI Model/Multimodal-Autonomous-Proxy-Application-v2"},"next":{"title":"AI large model online voice assistant","permalink":"/hh-101/HH-101/Online AI Model/AI-Large-Model-Online-Voice-Assistant"}}');var r=i(74848),s=i(28453);const o={title:"Online speech to text (ASR)",sidebar_position:12},a="1.Online speech to text (ASR)",l={},c=[{value:"1. Concept Introduction",id:"1-concept-introduction",level:2},{value:"1.1 What is &quot;ASR&quot;?",id:"11-what-is-asr",level:3},{value:"1.2 Implementation Principles",id:"12-implementation-principles",level:3},{value:"1. Acoustic Model",id:"1-acoustic-model",level:4},{value:"2. Language Model",id:"2-language-model",level:4},{value:"3. Pronunciation Dictionary",id:"3-pronunciation-dictionary",level:4},{value:"4. Decoder",id:"4-decoder",level:4},{value:"5. End-to-End ASR",id:"5-end-to-end-asr",level:4},{value:"2. Project Architecture",id:"2-project-architecture",level:2},{value:"Key Code",id:"key-code",level:3},{value:"1. Speech Processing and Recognition Core (largemodel/largemodel/asr.py)",id:"1-speech-processing-and-recognition-core-largemodellargemodelasrpy",level:4},{value:"2. VAD smart recording (largemodel/largemodel/asr.py)",id:"2-vad-smart-recording-largemodellargemodelasrpy",level:4},{value:"Code Analysis",id:"code-analysis",level:3},{value:"3. Practical Operations",id:"3-practical-operations",level:2},{value:"3.1 Configuring Online ASR",id:"31-configuring-online-asr",level:3},{value:"3.2 Starting and Testing the Function",id:"32-starting-and-testing-the-function",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"1online-speech-to-text-asr",children:"1.Online speech to text (ASR)"})}),"\n",(0,r.jsx)(n.h2,{id:"1-concept-introduction",children:"1. Concept Introduction"}),"\n",(0,r.jsx)(n.h3,{id:"11-what-is-asr",children:'1.1 What is "ASR"?'}),"\n",(0,r.jsx)(n.p,{children:'ASR (Automatic Speech Recognition) is a technology that converts human speech signals into text. It is widely used in intelligent assistants, voice command control, telephone customer service automation, and real-time subtitle generation. The goal of ASR is to enable machines to "understand" human speech and convert it into a form that computers can process and understand.'}),"\n",(0,r.jsx)(n.h3,{id:"12-implementation-principles",children:"1.2 Implementation Principles"}),"\n",(0,r.jsx)(n.p,{children:"The implementation of an ASR system primarily relies on the following key technical components:"}),"\n",(0,r.jsx)(n.h4,{id:"1-acoustic-model",children:"1. Acoustic Model"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The acoustic model is responsible for converting the input sound signal into phonemes or subword units. This typically involves feature extraction, such as using Mel-Frequency Cepstral Coefficients (MFCCs) or filter banks to represent the audio signal."}),"\n",(0,r.jsx)(n.li,{children:"These features are then fed into a deep neural network (DNN), convolutional neural network (CNN), recurrent neural network (RNN), or the more advanced Transformer architecture for training, learning how to map audio features to corresponding phonemes or words."}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"2-language-model",children:"2. Language Model"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A language model predicts the most likely next word given the previous context, thereby improving recognition accuracy. It is trained on large amounts of text data to understand which word sequences are most likely to occur."}),"\n",(0,r.jsx)(n.li,{children:"Common language models include n-gram models, RNN-based language models (LMs), and the more popular Transformer-based language models."}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"3-pronunciation-dictionary",children:"3. Pronunciation Dictionary"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"A pronunciation dictionary provides a mapping between words and their corresponding pronunciations. This is crucial for connecting the acoustic model and language model, as it allows the system to understand and match the sounds it hears to known pronunciation rules."}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"4-decoder",children:"4. Decoder"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"The decoder's task is to find the most likely word sequence as output given the acoustic model, language model, and pronunciation dictionary. This process typically involves complex search algorithms, such as the Viterbi algorithm or graph-based search methods, to find the optimal path."}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"5-end-to-end-asr",children:"5. End-to-End ASR"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"With the advancement of deep learning, end-to-end ASR systems have emerged. These systems attempt to learn text output directly from raw audio signals, without the need for explicit acoustic models, separate pronunciation lexicons, and language models. These systems are often based on sequence-to-sequence (Seq2Seq) frameworks, using, for example, attention mechanisms or the Transformer architecture, significantly simplifying the complexity of traditional ASR systems."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"In general, modern ASR systems achieve efficient and accurate human speech-to-text conversion by combining the aforementioned components and leveraging large-scale datasets and powerful computing resources for training. With technological advancements, the performance of ASR systems continues to improve, and their application scenarios are becoming increasingly broad."}),"\n",(0,r.jsx)(n.h2,{id:"2-project-architecture",children:"2. Project Architecture"}),"\n",(0,r.jsx)(n.h3,{id:"key-code",children:"Key Code"}),"\n",(0,r.jsx)(n.h4,{id:"1-speech-processing-and-recognition-core-largemodellargemodelasrpy",children:"1. Speech Processing and Recognition Core (largemodel/largemodel/asr.py)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# From largemodel/largemodel/asr.py\ndef kws_handler(self)->None:\nif self.stop_event.is_set():\nreturn\n\nif self.listen_for_speech(self.mic_index):\nasr_text = self.ASR_conversion(self.user_speechdir)  # Perform ASR conversion\nif asr_text =='error':  # Check if ASR result length is less than 4 characters\nself.get_logger().warn(\"I still don't understand what you mean. Please try again\")\nplaysound(self.audio_dict[self.error_response])  # Error response\nelse:\nself.get_logger().info(asr_text)\nself.get_logger().info(\"okay\ud83d\ude00, let me think for a moment...\")\nself.asr_pub_result(asr_text)  # Publish ASR result\nelse:\nreturn\n\ndef ASR_conversion(self, input_file:str)->str:\nif self.use_oline_asr:\nresult=self.modelinterface.oline_asr(input_file)\nif result[0] == 'ok' and len(result[1]) > 4:\nreturn result[1]\nelse:\nself.get_logger().error(f'ASR Error:{result[1]}')  # ASR error.\nreturn 'error'\nelse:\nresult=self.modelinterface.SenseVoiceSmall_ASR(input_file)\nif result[0] == 'ok' and len(result[1]) > 4:\nreturn result[1]\nelse:\nself.get_logger().error(f'ASR Error:{result[1]}')  # ASR error.\nreturn 'error'\n"})}),"\n",(0,r.jsx)(n.h4,{id:"2-vad-smart-recording-largemodellargemodelasrpy",children:"2. VAD smart recording (largemodel/largemodel/asr.py)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# From largemodel/largemodel/asr.py\ndef listen_for_speech(self,mic_index=0):\np = pyaudio.PyAudio()   # Create PyAudio instance.\naudio_buffer = []       # Store audio data.\nsilence_counter = 0     # Silence counter.\nMAX_SILENCE_FRAMES = 90  # Stop after 900ms of silence (30 frames * 30ms)\nspeaking = False  # Flag indicating speech activity.\nframe_counter = 0  # Frame counter.\nstream_kwargs = {\n'format': pyaudio.paInt16,\n'channels': 1,\n'rate': self.sample_rate,\n'input': True,\n'frames_per_buffer': self.frame_bytes,\n}\nif mic_index != 0:\nstream_kwargs['input_device_index'] = mic_index\n\n# Prompt the user to speak via the buzzer.\nself.pub_beep.publish(UInt16(data = 1))\ntime.sleep(0.5)\nself.pub_beep.publish(UInt16(data = 0))\n\ntry:\n# Open audio stream.\nstream = p.open(**stream_kwargs)\nwhile True:\nif self.stop_event.is_set():\nreturn False\n\nframe = stream.read(self.frame_bytes, exception_on_overflow=False)  # Read audio data.\nis_speech = self.vad.is_speech(frame, self.sample_rate)  # VAD detection.\n\nif is_speech:\n# Detected speech activity.\nspeaking = True\naudio_buffer.append(frame)\nsilence_counter = 0\nelse:\nif speaking:\n# Detect silence after speech activity.\nsilence_counter += 1\naudio_buffer.append(frame)  # Continue recording buffer.\n\n# End recording when silence duration meets the threshold.\nif silence_counter >= MAX_SILENCE_FRAMES:\nbreak\nframe_counter += 1\nif frame_counter % 2 == 0:\nself.get_logger().info('1' if is_speech else '-')\n# Real-time status display.\nfinally:\nstream.stop_stream()\nstream.close()\np.terminate()\n\n# Save valid recording (remove trailing silence).\nif speaking and len(audio_buffer) > 0:\n# Trim the last silent part.\nclean_buffer = audio_buffer[:-MAX_SILENCE_FRAMES] if len(audio_buffer) > MAX_SILENCE_FRAMES else audio_buffer\n\nwith wave.open(self.user_speechdir, 'wb') as wf:\nwf.setnchannels(1)\nwf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\nwf.setframerate(self.sample_rate)\nwf.writeframes(b''.join(clean_buffer))\nreturn True\n"})}),"\n",(0,r.jsx)(n.h3,{id:"code-analysis",children:"Code Analysis"}),"\n",(0,r.jsxs)(n.p,{children:["ASR (speech-to-text) functionality is provided by the",(0,r.jsx)(n.code,{children:"ASRNode"}),"node (",(0,r.jsx)(n.code,{children:"asr.py"}),"). This node is responsible for recording, converting, and publishing audio."]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Audio Recording ( listen_for_speech ): This function uses the pyaudio library to capture the audio stream from the microphone. It integrates the webrtcvad library for voice activity detection (VAD). The function loops through audio frames and uses vad.is_speech() to determine whether each frame contains human voice. When speech is detected, the data is written to a buffer. Recording stops when sustained silence (defined by MAX_SILENCE_FRAMES ) is detected. Finally, the audio data in the buffer is written to a .wav file in the self.user_speechdir directory."}),"\n",(0,r.jsx)(n.li,{children:"Backend Selection and Execution ( ASR_conversion ): The kws_handler function calls the ASR_conversion function after successful recording. This function determines which backend implementation to call by reading the ROS parameter use_oline_asr (a Boolean value). If false , the self.modelinterface.SenseVoiceSmall_ASR method is called for local recognition. If true , the self.modelinterface.oline_asr method is called for online recognition. This function passes the audio file path as a parameter to the selected method and processes the returned result."}),"\n",(0,r.jsx)(n.li,{children:"Result Publishing ( asr_pub_result ): After ASR_conversion returns valid text, kws_handler calls the asr_pub_result function. This function wraps a text string in a std_msgs.msg.String message and publishes it to the /asr topic via the ROS publisher."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"3-practical-operations",children:"3. Practical Operations"}),"\n",(0,r.jsx)(n.h3,{id:"31-configuring-online-asr",children:"3.1 Configuring Online ASR"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Open the main configuration file HemiHex.yaml :"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"gedit ~/yahboom_ws/src/largemodel/config/yahboom.yaml\n"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Enable Online ASR Mode : Set the use_oline_asr parameter to True . xxxxxxxxxx # HemiHex.yaml asr : ros__parameters : use_oline_asr : True # Key: Change from False to True language : 'en' regional_setting : \"international\""}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"32-starting-and-testing-the-function",children:"3.2 Starting and Testing the Function"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Startup Command : xxxxxxxxxx ros2 launch largemodel asr_only.launch.py"}),"\n",(0,r.jsx)(n.li,{children:'Test : Say "Hi, HemiHex" into the microphone, and it will respond, "I\'m here." Then you can start talking. Finally, the recorded audio will be converted into text and printed on the terminal.'}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var t=i(96540);const r={},s=t.createContext(r);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);