"use strict";(globalThis.webpackChunkhemihex_docs=globalThis.webpackChunkhemihex_docs||[]).push([[5602],{15197:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/image-20250801214649785-6927b3240656826aaa941dcd1077f489.png"},28116:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>o});const s=JSON.parse('{"id":"HH-101/Offline AI Model/multimodal-text-and-graphics-application-11-offlineaimodel-11-21","title":"Multimodal Text and Graphics Application","description":"Since Ollama does not support the text-based image function, we need to use other tools to implement the local text-based image function. Currently, voice control of the text-based image function is not supported, so the content of this article is the same as the offline text multimodal text-based image application.","source":"@site/docs/hh101/HH-101/11 - Offline AI Model/21-multimodal-text-and-graphics-application-11-offlineaimodel-11-21.md","sourceDirName":"HH-101/11 - Offline AI Model","slug":"/HH-101/Offline AI Model/multimodal-text-and-graphics-application-11-offlineaimodel-11-21","permalink":"/hh-101/HH-101/Offline AI Model/multimodal-text-and-graphics-application-11-offlineaimodel-11-21","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"title":"Multimodal Text and Graphics Application","sidebar_position":0},"sidebar":"hh101Sidebar","previous":{"title":"Multimodal Visual Understanding Speech Interaction","permalink":"/hh-101/HH-101/Offline AI Model/multimodal-visual-understanding-speech-interaction-11-offlineaimodel-11-20"},"next":{"title":"Multimodal Video Analysis Application","permalink":"/hh-101/HH-101/Offline AI Model/multimodal-video-analysis-application-11-offlineaimodel-11-22"}}');var i=a(74848),t=a(28453);const l={title:"Multimodal Text and Graphics Application",sidebar_position:0},r="Multimodal Text and Graphics Application",c={},o=[{value:"1. Concept Introduction",id:"1-concept-introduction",level:2},{value:"1.1 What is Text-to-Image?",id:"11-what-is-text-to-image",level:3},{value:"Core Principles",id:"core-principles",level:4},{value:"1.2 What is FastSDCPU?",id:"12-what-is-fastsdcpu",level:3},{value:"Core Features",id:"core-features",level:4},{value:"Applicable Scenarios",id:"applicable-scenarios",level:4},{value:"2. Project Deployment",id:"2-project-deployment",level:2},{value:"2.1 Deployment Environment",id:"21-deployment-environment",level:3},{value:"2.2 LAN Access",id:"22-lan-access",level:3},{value:"2.3 Using the Vincent Image Function",id:"23-using-the-vincent-image-function",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",p:"p",pre:"pre",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"multimodal-text-and-graphics-application",children:"Multimodal Text and Graphics Application"})}),"\n",(0,i.jsx)(n.p,{children:"Since Ollama does not support the text-based image function, we need to use other tools to implement the local text-based image function. Currently, voice control of the text-based image function is not supported, so the content of this article is the same as the offline text multimodal text-based image application."}),"\n",(0,i.jsx)(n.h2,{id:"1-concept-introduction",children:"1. Concept Introduction"}),"\n",(0,i.jsx)(n.h3,{id:"11-what-is-text-to-image",children:"1.1 What is Text-to-Image?"}),"\n",(0,i.jsx)(n.p,{children:'Text-to-Image is an AI technology that automatically generates corresponding images based on text description . Simply input a piece of text (for example, "A Shiba Inu wearing sunglasses surfing on the beach"), and the AI \u200b\u200bmodel will generate an image that matches the description based on semantic understanding, without any drawing or design knowledge required.'}),"\n",(0,i.jsx)(n.h4,{id:"core-principles",children:"Core Principles"}),"\n",(0,i.jsx)(n.h3,{id:"12-what-is-fastsdcpu",children:"1.2 What is FastSDCPU?"}),"\n",(0,i.jsx)(n.p,{children:"FastSDCPU is an open-source Stable Diffusion image processing project optimized for CPU devices. Through algorithmic and engineering optimizations, it enables rapid generation of high-quality images on standard computers without GPUs, significantly lowering the hardware barrier to entry for AI painting."}),"\n",(0,i.jsx)(n.h4,{id:"core-features",children:"Core Features"}),"\n",(0,i.jsx)(n.h4,{id:"applicable-scenarios",children:"Applicable Scenarios"}),"\n",(0,i.jsx)(n.h2,{id:"2-project-deployment",children:"2. Project Deployment"}),"\n",(0,i.jsx)(n.h3,{id:"21-deployment-environment",children:"2.1 Deployment Environment"}),"\n",(0,i.jsx)(n.p,{children:"Open a terminal and execute the following code:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# If Git is not installed on your system, run:\r\n\u200b\r\nsudo apt update\r\n\u200b\r\nsudo apt install git -y\r\n\u200b\r\nsudo apt install python3.10-venv -y\r\n\u200b\r\n# Add environment variables\r\n\u200b\r\necho 'export PATH=\"$HOME/.local/bin:$PATH\"' >> ~/.bashrc\r\n\u200b\r\nsource ~/.bashrc\r\n\u200b\r\n# Clone the project code\r\n\u200b\r\ngit clone https://github.com/rupeshs/fastsdcpu.git\r\n\u200b\r\ncd fastsdcpu\r\n\u200b\r\n# Create a virtual environment and install dependencies\r\n\u200b\r\npython -m venv venv\r\n\u200b\r\nsource venv/bin/activate\r\n\u200b\r\n# Install uv\r\n\u200b\r\ncurl -Ls https://astral.sh/uv/install.sh | sh (This step may fail if you don't have a proxy in China. If not, skip this step and proceed to the next command.)\r\n\u200b\r\n#If the curl command in the previous step failed to install uv, run these three commands:\r\nwget https://mirrors.huaweicloud.com/astral/uv/0.8.4/uv-aarch64-unknown-linux-gnu -O ~/.local/bin/uv\r\nchmod +x ~/.local/bin/uv\r\n~/.local/bin/uv --version\r\n\u200b\r\nchmod +x install.sh start-webui.sh\r\n./install.sh --disable-gui\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# If Git is not installed on your system, run:\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"sudo apt update\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"sudo apt install git -y\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"sudo apt install python3.10-venv -y\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Add environment variables\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"echo 'export PATH=\"$HOME/.local/bin:$PATH\"' >> ~/.bashrc\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"source ~/.bashrc\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Clone the project code\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/rupeshs/fastsdcpu.git\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd fastsdcpu\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Create a virtual environment and install dependencies\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python -m venv venv\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"source venv/bin/activate\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install uv\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"curl -Ls https://astral.sh/uv/install.sh | sh (This step may fail if you don't have a proxy in China. If not, skip this step and proceed to the next command.)\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"#If the curl command in the previous step failed to install uv, run these three commands:\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"wget https://mirrors.huaweicloud.com/astral/uv/0.8.4/uv-aarch64-unknown-linux-gnu -O ~/.local/bin/uv\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"chmod +x ~/.local/bin/uv\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"~/.local/bin/uv --version\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"chmod +x install.sh start-webui.sh\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./install.sh --disable-gui\n"})}),"\n",(0,i.jsx)(n.p,{children:"Installation successful. Press any key to exit:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image-20250801214649785",src:a(15197).A+"",width:"544",height:"290"})}),"\n",(0,i.jsx)(n.h3,{id:"22-lan-access",children:"2.2 LAN Access"}),"\n",(0,i.jsx)(n.p,{children:"Before starting, you need to modify a file to support LAN access. Otherwise, the webui can only be accessed locally:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\r\nvim ~/fastsdcpu/src/frontend/webui/ui.py\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"vim ~/fastsdcpu/src/frontend/webui/ui.py\n"})}),"\n",(0,i.jsx)(n.p,{children:'After opening the ui.py file, scroll to the last line and find webui.launch(share=share) . Change it to webui.launch(server_name="0.0.0.0",share=share)'}),"\n",(0,i.jsx)(n.p,{children:"Save the file."}),"\n",(0,i.jsx)(n.p,{children:"Start:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"\r\n./start-webui.sh\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./start-webui.sh\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image-20250801221311739",src:a(54226).A+"",width:"955",height:"626"})}),"\n",(0,i.jsx)(n.p,{children:"You can then access the webui by entering your motherboard's IP address: 7860 in your browser."}),"\n",(0,i.jsx)(n.h3,{id:"23-using-the-vincent-image-function",children:"2.3 Using the Vincent Image Function"}),"\n",(0,i.jsx)(n.p,{children:"Use ifconfig in the terminal to query the motherboard's IP address. For example, mine is 192.168.2.106."}),"\n",(0,i.jsx)(n.p,{children:"Then open a browser and enter your motherboard's IP address:7860 . For example, I entered 192.168.2.106:7860. This will access the web UI."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image-20250804105134704",src:a(86195).A+"",width:"1592",height:"966"})}),"\n",(0,i.jsx)(n.p,{children:"Next, click LCM-LoRA. This model uses less memory. If you want to use a different model, feel free to research it yourself."}),"\n",(0,i.jsx)(n.p,{children:"Next, click Models to view the LCM-LoRA model settings. You can change the model to your preference, or just stick with the default, as I did."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image-20250804105439656",src:a(94691).A+"",width:"1536",height:"580"})}),"\n",(0,i.jsx)(n.p,{children:"Next, click Generation Settings and increase the Inference Steps setting to improve the quality of the generated image. I've set it to 5."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image-20250804111046973",src:a(63898).A+"",width:"1532",height:"534"})}),"\n",(0,i.jsx)(n.p,{children:"Next, return to the Text to Image dialog box and enter the content you want to generate. Click Generate to begin generating the image."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image-20250804105656393",src:a(28530).A+"",width:"1501",height:"818"})}),"\n",(0,i.jsx)(n.p,{children:"For first-time users, you'll need to download the model. You'll see the default model being downloaded in the terminal. Once it's finished downloading, the text-to-image function will begin."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image-20250804105710057",src:a(33300).A+"",width:"955",height:"626"})}),"\n",(0,i.jsx)(n.p,{children:"Generated result:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"image-20250804113118977",src:a(35024).A+"",width:"1467",height:"839"})}),"\n",(0,i.jsx)(n.p,{children:"This project has improved support for English, and the generated images will be more consistent with the text. We recommend using English descriptions when generating images."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},28453:(e,n,a)=>{a.d(n,{R:()=>l,x:()=>r});var s=a(96540);const i={},t=s.createContext(i);function l(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),s.createElement(t.Provider,{value:n},e.children)}},28530:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/image-20250804105656393-1f3a96a48c20e306c6dc3144ab26d1ef.png"},33300:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/image-20250804105710057-75abd137e49259669a18bb9b4e538a00.png"},35024:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/image-20250804113118977-94775d003d1fae675c5c2a6fe8511a08.png"},54226:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/image-20250801221311739-74a6c523f98f1ec3e7a963e2371aa4ee.png"},63898:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/image-20250804111046973-1daed581749470742bcb22cee031be85.png"},86195:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/image-20250804105134704-14783deec13365798e1b98fbd6c8722c.png"},94691:(e,n,a)=>{a.d(n,{A:()=>s});const s=a.p+"assets/images/image-20250804105439656-867cd484baafb3881e67666e4b74ed16.png"}}]);