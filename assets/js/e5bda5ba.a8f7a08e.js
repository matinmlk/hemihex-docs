"use strict";(globalThis.webpackChunkhemihex_docs=globalThis.webpackChunkhemihex_docs||[]).push([[5342],{10547:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>t,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>c});const l=JSON.parse('{"id":"HH-101/Offline AI Model/multi-module-visual-position-application-11-offlineaimodel-11-23","title":"Multi-module Visual Position Application","description":"1. Introduction","source":"@site/docs/hh101/HH-101/11- Offline AI Model/23-multi-module-visual-position-application-11-offlineaimodel-11-23.md","sourceDirName":"HH-101/11- Offline AI Model","slug":"/HH-101/Offline AI Model/multi-module-visual-position-application-11-offlineaimodel-11-23","permalink":"/hh-101/HH-101/Offline AI Model/multi-module-visual-position-application-11-offlineaimodel-11-23","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":0,"frontMatter":{"title":"Multi-module Visual Position Application","sidebar_position":0},"sidebar":"hh101Sidebar","previous":{"title":"Multimodal Video Analysis Application","permalink":"/hh-101/HH-101/Offline AI Model/multimodal-video-analysis-application-11-offlineaimodel-11-22"},"next":{"title":"Multimodal Table Scanning Application","permalink":"/hh-101/HH-101/Offline AI Model/multimodal-table-scanning-application-11-offlineaimodel-11-24"}}');var s=a(74848),i=a(28453);const o={title:"Multi-module Visual Position Application",sidebar_position:0},r="Multi module visual position application",t={},c=[{value:"1. Introduction",id:"1-introduction",level:2},{value:"1.1 What is &quot;Multimodal Visual Localization&quot;?",id:"11-what-is-multimodal-visual-localization",level:3},{value:"1.2 Overview of Implementation Principles",id:"12-overview-of-implementation-principles",level:3},{value:"2. Code Analysis",id:"2-code-analysis",level:2},{value:"Key Code",id:"key-code",level:3},{value:"1. Tools Layer Entry ( largemodel/utils/tools_manager.py )",id:"1-tools-layer-entry--largemodelutilstools_managerpy-",level:4},{value:"2. Model interface layer ( largemodel/utils/large_model_interface.py )",id:"2-model-interface-layer--largemodelutilslarge_model_interfacepy-",level:4},{value:"Code Analysis",id:"code-analysis",level:3},{value:"3. Practical Application",id:"3-practical-application",level:2},{value:"3.1 Configuring the Offline Large-Scale Model",id:"31-configuring-the-offline-large-scale-model",level:3},{value:"3.1.1 Configuring the LLM Platform (HemiHex.yaml)",id:"311-configuring-the-llm-platform-hemihexyaml",level:4},{value:"3.1.2 Configuration model interface ( large_model_interface.yaml )",id:"312-configuration-model-interface--large_model_interfaceyaml-",level:4},{value:"3.2 Starting and Testing the Function",id:"32-starting-and-testing-the-function",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"multi-module-visual-position-application",children:"Multi module visual position application"})}),"\n",(0,s.jsx)(n.h2,{id:"1-introduction",children:"1. Introduction"}),"\n",(0,s.jsx)(n.h3,{id:"11-what-is-multimodal-visual-localization",children:'1.1 What is "Multimodal Visual Localization"?'}),"\n",(0,s.jsx)(n.p,{children:"Multimodal visual localization is a technology that combines multiple sensor inputs (such as cameras, depth sensors, and IMUs) with algorithmic processing techniques to accurately identify and track the position and posture of a device or user in an environment. This technology does not rely solely on a single type of sensor data, but instead integrates information from different perception modalities, thereby improving localization accuracy and robustness."}),"\n",(0,s.jsx)(n.h3,{id:"12-overview-of-implementation-principles",children:"1.2 Overview of Implementation Principles"}),"\n",(0,s.jsx)(n.h2,{id:"2-code-analysis",children:"2. Code Analysis"}),"\n",(0,s.jsx)(n.h3,{id:"key-code",children:"Key Code"}),"\n",(0,s.jsx)(n.h4,{id:"1-tools-layer-entry--largemodelutilstools_managerpy-",children:"1. Tools Layer Entry ( largemodel/utils/tools_manager.py )"}),"\n",(0,s.jsx)(n.p,{children:"The visual_positioning function in this file defines the execution flow of the tool, specifically how it constructs a prompt containing the target object name and formatting requirements."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'xxxxxxxxxx\n# From largemodel/utils/tools_manager.py\nclass\nToolsManager\n:\n# ...\ndef\nvisual_positioning\n(\nself\n,\nargs\n):\n"""\nLocate object coordinates in image and save results to MD file.\n\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7269\u4f53\u5750\u6807\u5e76\u5c06\u7ed3\u679c\u4fdd\u5b58\u4e3aMD\u6587\u4ef6\u3002\n:param args: Arguments containing image path and object name.\n:return: Dictionary with file path and coordinate data.\n"""\nself\n.\nnode\n.\nget_logger\n().\ninfo\n(\nf"Executing visual_positioning() tool with args: {args}"\n)\ntry\n:\nimage_path\n=\nargs\n.\nget\n(\n"image_path"\n)\nobject_name\n=\nargs\n.\nget\n(\n"object_name"\n)\n# ... (Path fallback mechanism and parameter checking)\n# Construct a prompt asking the large model to identify the coordinates of the specified object. / \u6784\u9020\u63d0\u793a\uff0c\u8981\u6c42\u5927\u6a21\u578b\u8bc6\u522b\u6307\u5b9a\u7269\u54c1\u7684\u5750\u6807\u3002\nif\nself\n.\nnode\n.\nlanguage\n==\n\'zh\'\n:\nprompt\n=\nf"\u8bf7\u4ed4\u7ec6\u5206\u6790\u8fd9\u5f20\u56fe\u7247\uff0c\u7528\u4e00\u4e2a\u4e2a\u6846\u5b9a\u4f4d\u56fe\u50cf\u6bcf\u4e00\u4e2a{object_name}\u7684\u4f4d\u7f6e..."\nelse\n:\nprompt\n=\nf"Please carefully analyze this image and find the position of all {object_name}..."\n# ... (Building an independent message context)\nresult\n=\nself\n.\nnode\n.\nmodel_client\n.\ninfer_with_image\n(\nimage_path\n,\nprompt\n,\nmessage\n=\nmessage_to_use\n)\n# ... (Process and parse the returned coordinate text)\nreturn\n{\n"file_path"\n:\nmd_file_path\n,\n"coordinates_content"\n:\ncoordinates_content\n,\n"explanation_content"\n:\nexplanation_content\n}\n# ... (Error Handling)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# From largemodel/utils/tools_manager.py\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"class\nToolsManager\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ...\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"def\nvisual_positioning\n(\nself\n,\nargs\n):\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"""\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Locate object coordinates in image and save results to MD file.\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7269\u4f53\u5750\u6807\u5e76\u5c06\u7ed3\u679c\u4fdd\u5b58\u4e3aMD\u6587\u4ef6\u3002\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:":param args: Arguments containing image path and object name.\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:":return: Dictionary with file path and coordinate data.\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"""\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'self\n.\nnode\n.\nget_logger\n().\ninfo\n(\nf"Executing visual_positioning() tool with args: {args}"\n)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"try\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'image_path\n=\nargs\n.\nget\n(\n"image_path"\n)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'object_name\n=\nargs\n.\nget\n(\n"object_name"\n)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (Path fallback mechanism and parameter checking)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Construct a prompt asking the large model to identify the coordinates of the specified object. / \u6784\u9020\u63d0\u793a\uff0c\u8981\u6c42\u5927\u6a21\u578b\u8bc6\u522b\u6307\u5b9a\u7269\u54c1\u7684\u5750\u6807\u3002\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"if\nself\n.\nnode\n.\nlanguage\n==\n'zh'\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'prompt\n=\nf"\u8bf7\u4ed4\u7ec6\u5206\u6790\u8fd9\u5f20\u56fe\u7247\uff0c\u7528\u4e00\u4e2a\u4e2a\u6846\u5b9a\u4f4d\u56fe\u50cf\u6bcf\u4e00\u4e2a{object_name}\u7684\u4f4d\u7f6e..."\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"else\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'prompt\n=\nf"Please carefully analyze this image and find the position of all {object_name}..."\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (Building an independent message context)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"result\n=\nself\n.\nnode\n.\nmodel_client\n.\ninfer_with_image\n(\nimage_path\n,\nprompt\n,\nmessage\n=\nmessage_to_use\n)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (Process and parse the returned coordinate text)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"return\n{\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"file_path"\n:\nmd_file_path\n,\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"coordinates_content"\n:\ncoordinates_content\n,\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"explanation_content"\n:\nexplanation_content\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"}\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (Error Handling)\n"})}),"\n",(0,s.jsx)(n.h4,{id:"2-model-interface-layer--largemodelutilslarge_model_interfacepy-",children:"2. Model interface layer ( largemodel/utils/large_model_interface.py )"}),"\n",(0,s.jsx)(n.p,{children:"The infer_with_image function in this file is the unified entry point for all image-related tasks."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n# From largemodel/utils/large_model_interface.py\n\u200b\nclass\nmodel_interface\n:\n# ...\ndef\ninfer_with_image\n(\nself\n,\nimage_path\n,\ntext\n=\nNone\n,\nmessage\n=\nNone\n):\n\"\"\"Unified image inference interface. / \u7edf\u4e00\u7684\u56fe\u50cf\u63a8\u7406\u63a5\u53e3\u3002\"\"\"\n# ... (Prepare Message)\ntry\n:\n# Determine which specific implementation to call based on the value of self.llm_platform\nif\nself\n.\nllm_platform\n==\n'ollama'\n:\nresponse_content\n=\nself\n.\nollama_infer\n(\nself\n.\nmessages\n,\nimage_path\n=\nimage_path\n)\nelif\nself\n.\nllm_platform\n==\n'tongyi'\n:\n# ... Logic for calling the Tongyi model\npass\n# ... (Logic of other platforms)\n# ...\nreturn\n{\n'response'\n:\nresponse_content\n,\n'messages'\n:\nself\n.\nmessages\n.\ncopy\n()}\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# From largemodel/utils/large_model_interface.py\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"class\nmodel_interface\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ...\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"def\ninfer_with_image\n(\nself\n,\nimage_path\n,\ntext\n=\nNone\n,\nmessage\n=\nNone\n):\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'"""Unified image inference interface. / \u7edf\u4e00\u7684\u56fe\u50cf\u63a8\u7406\u63a5\u53e3\u3002"""\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (Prepare Message)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"try\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Determine which specific implementation to call based on the value of self.llm_platform\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"if\nself\n.\nllm_platform\n==\n'ollama'\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"response_content\n=\nself\n.\nollama_infer\n(\nself\n.\nmessages\n,\nimage_path\n=\nimage_path\n)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"elif\nself\n.\nllm_platform\n==\n'tongyi'\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... Logic for calling the Tongyi model\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pass\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ... (Logic of other platforms)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# ...\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"return\n{\n'response'\n:\nresponse_content\n,\n'messages'\n:\nself\n.\nmessages\n.\ncopy\n()}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"code-analysis",children:"Code Analysis"}),"\n",(0,s.jsx)(n.p,{children:"The core of the visual positioning function lies in guiding large models to output structured data through precise instructions . It also follows the layered design of the tool layer and the model interface layer."}),"\n",(0,s.jsx)(n.p,{children:"In summary, the general workflow for visual localization is: ToolsManager receives the target object name and constructs a precise prompt requesting coordinates. ToolsManager calls the model interface. ModelInterface packages the image and prompt together and sends them to the corresponding model platform according to the configuration. The model returns text containing the coordinates. ModelInterface returns this text to ToolsManager. ToolsManager parses the text, extracts the structured coordinate data, and returns it. This process demonstrates how prompt engineering techniques can be used to enable a general-purpose large-scale visual model to accomplish more specific and structured tasks."}),"\n",(0,s.jsx)(n.h2,{id:"3-practical-application",children:"3. Practical Application"}),"\n",(0,s.jsx)(n.h3,{id:"31-configuring-the-offline-large-scale-model",children:"3.1 Configuring the Offline Large-Scale Model"}),"\n",(0,s.jsx)(n.h4,{id:"311-configuring-the-llm-platform-hemihexyaml",children:"3.1.1 Configuring the LLM Platform (HemiHex.yaml)"}),"\n",(0,s.jsx)(n.p,{children:"This file determines which large-scale model platform the model_service node loads as its primary language model."}),"\n",(0,s.jsx)(n.p,{children:"Open the file in the terminal :"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\nvim\n~/yahboom_ws/src/largemodel/config/HemiHex.yaml\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"vim\n~/yahboom_ws/src/largemodel/config/HemiHex.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:"Modify/Confirm llm_platform :"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\nmodel_service\n:\n#Model server node parameters\nros__parameters\n:\nlanguage\n:\n'en'\n#Large Model Interface Language\nuseolinetts\n:\nTrue\n#This item is invalid in text mode and can be ignored\n\u200b\n# Large model configuration\nllm_platform\n:\n'ollama'\n# Key: Make sure it's 'ollama'\nregional_setting\n:\n\"international\"\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"model_service\n:\n#Model server node parameters\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros__parameters\n:\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"language\n:\n'en'\n#Large Model Interface Language\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"useolinetts\n:\nTrue\n#This item is invalid in text mode and can be ignored\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"\u200b\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Large model configuration\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"llm_platform\n:\n'ollama'\n# Key: Make sure it's 'ollama'\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'regional_setting\n:\n"international"\n'})}),"\n",(0,s.jsx)(n.h4,{id:"312-configuration-model-interface--large_model_interfaceyaml-",children:"3.1.2 Configuration model interface ( large_model_interface.yaml )"}),"\n",(0,s.jsx)(n.p,{children:"This file defines which visual model to use when the platform is selected as ollama ."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\nvim ~/yahboom_ws/src/largemodel/config/large_model_interface.yaml\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"vim ~/yahboom_ws/src/largemodel/config/large_model_interface.yaml\n"})}),"\n",(0,s.jsx)(n.p,{children:"2.Find the ollama related configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'xxxxxxxxxx\n#.....\n## \u79bb\u7ebf\u5927\u6a21\u578b (Offline Large Language Models)\n# Ollama Configuration\nollama_host:\n"http://localhost:11434"\n# Ollama server address\nollama_model:\n"llava"\n# Key: Change this to the multimodal model you downloaded, such as "llava"\n#.....\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"#.....\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"## \u79bb\u7ebf\u5927\u6a21\u578b (Offline Large Language Models)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Ollama Configuration\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'ollama_host:\n"http://localhost:11434"\n# Ollama server address\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'ollama_model:\n"llava"\n# Key: Change this to the multimodal model you downloaded, such as "llava"\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"#.....\n"})}),"\n",(0,s.jsx)(n.p,{children:"Note : Please ensure that the model specified in the configuration parameters (e.g., llava ) can handle multimodal input."}),"\n",(0,s.jsx)(n.h3,{id:"32-starting-and-testing-the-function",children:"3.2 Starting and Testing the Function"}),"\n",(0,s.jsx)(n.p,{children:"Note: Due to performance limitations, this example cannot be run on the Jetson Orin Nano 4GB. To experience this feature, please refer to the corresponding section in [Online Large Model (Voice Interaction)]"}),"\n",(0,s.jsx)(n.p,{children:"Prepare image files :"}),"\n",(0,s.jsx)(n.p,{children:"Place an image file to test in the following path: /home/jetson/yahboom_ws/src/largemodel/resources_file/visual_positioning"}),"\n",(0,s.jsx)(n.p,{children:"Then name the image test_image.jpg"}),"\n",(0,s.jsx)(n.p,{children:"Start largemodel main program :"}),"\n",(0,s.jsx)(n.p,{children:"Open a terminal and run the following command:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\nros2 launch largemodel largemodel_control.launch.py\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"xxxxxxxxxx\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch largemodel largemodel_control.launch.py\n"})}),"\n",(0,s.jsx)(n.p,{children:"Test :"})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>r});var l=a(96540);const s={},i=l.createContext(s);function o(e){const n=l.useContext(i);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),l.createElement(i.Provider,{value:n},e.children)}}}]);